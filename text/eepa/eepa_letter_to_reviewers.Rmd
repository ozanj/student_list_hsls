---
title: "Response to Editors and Reviewers"
author: "ozan"
date: "`r Sys.Date()`"
output: pdf_document
bibliography: ../../assets/bib/eepa_student_list.bib
csl: ../../assets/bib/apa.csl
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Editor 

## Major recommendations/concerns

1. College admissions priorities or student list products

> **Editor concern**. *First, Reviewer 1 and 2 note that several of the arguments made in the paper are not necessarily about college lists, but instead about the admissions priorities of each institution. We believe it would be useful to tease this out more in the paper. We also believe it would be helpful to provide some (masked) information about the 14 institutions. This could be rounded enrollment numbers (in bands) or Carnegie Classification or something else. But it feels like as the argument is strengthened on why this might be about the admissions priorities of institutions, the reader would benefit from better understanding the contexts of the 14 institutions.*

**Author response**: We agree with these concerns raised by Reviewers 1 and 2. We read Reviewer 1's concern as including admissions priorities, but being broader than admissions priorities (The first major concern of Reviewer 1 is "where does the problem lie" and Reviewer 1 states "The nature of the [public records] request made me think we're dealing with how universities use student requests either implicitly or explicitly to privilege certain groups in the search and recruitment process").

Thus, the revised manuscript reframed the problem with student lists as (1) the underlying architecture of student list products and (2) how student list products are utilized by college administrators executing student list purchases. Alongside this reframing, the revised manuscript is now motivated by three research questions:

- RQ1: What is relationship between individual search filters and racial composition of included vs excluded students? 
- RQ2: In what ways do public universities utilize racialized input search filters in concert with other search filters when purchasing student lists? 
- RQ3: What is racial composition of student list purchases that utilize racialized input search filters in concert with other search filters? 

RQ2 reflects the revised emphasis on how student list products are utilized in practice. The revised conceptual framework now concludes with a subsection named "Utilizing Student List Products" that draws from the literature on product utilization from sociology to inform targeted analyses of actual student lists purchased by public universities. This literature states that product utilization is partly a function of occupational/professional norms, which motivates the idea that criteria selected by individual colleges are likely to reflect college admissions standards. However, the utilization literature raises additional key factors that can yield racial inequality (e.g., implicit bias by individual decision-makers). In particular, we state that "Americans dramatically underestimate the magnitude of racial income inequality (Kraus, Onyeador, Daumeyer, Rucker, & Richeson, 2019). Therefore, discretionary selection devices that incorporate racialized inputs may produce racial inequality because decision-makers may be ignorant about how
these inputs interact with local patterns of racial inequality (Korver-Glenn, 2018)." These ideas raise the possibility that racial inequality in student list purchases may be the result of decision-makers having incomplete knowledge about how a complicated product (e.g., multiple intersecting fitlers) interacts with local racial segregation.

We believe that this broader emphasis on "product utilization" -- including admissions standards -- is preferable because focusing solely on admissions standards makes the implicit assumption that racial inequality in student list purchases are primarily a function of conscious decisions with respect to admissions standards, but this is inconsistent with findings from the literature on product utilization.

However, the revised manuscript acknowledges that admissions standards are likely an important driver of student list purchases. The revised manuscript includes results which show that public research universities in our data collection sample were more likely to condition on test scores (as opposed to GPA) and utilized higher test-score thresholds than public MA/doctoral universities. We feel that these findings are sufficient to drive home the point that admissions standards matter. We do not provide detailed information about Carnegie Classification/selectivity/admissions standards we did not feel that such information is essential given space limitations. However, this information could be added if Editors/Reviewers feel it is essential.

2. Empirical analyses do not make a contribution

> **Editor concern**. *Second, Reviewer 1 notes that the current analysis does not provide a substantial contribution to our understanding of admissions recruitment.The reviewer provides suggestions of ways the authors could expand the analysis to provide substantial insight into recruitment. We are not being prescriptive to say that the authors must follow Reviewer 1’s guidance. However, there needs to be more done. The current findings generally appear to align with what we already know about who takes college entrance exams or has higher academic GPAs. The current tests of proportions are not that revelatory given that the appendix tables show that almost every estimate is statistically significant (likely partially due to Type I error but also because these are large samples that will find practically insignificant differences statistically significant). More interesting are the findings that combining these elements creates a substantially more exclusive prospect pool (more could be done to explicitly tease this out). This also applies more specifically to the discussion section. Arguments are put forward that “several academic filters and geographic filters are structurally racist inputs” yet there is no clear explanation of when an academic filter would be a structurally racist input and when one would not be. For example, if an institution actually did use GPA and zipcode to expand the diversity of the prospect pool, is that a structurally racist input?If so, can structurally racist inputs be used for good? These are the more interesting, and challenging questions, that the paper could engage in.This would then also allow the authors to provide clearer policy recommendations.*

**Author response**. We push back by arguing that this manuscript makes a contribution that differs from a purely empirical contribution. We believe that the primary contributions of this manuscript are not empirical, but rather (1) motivating future research on topics that have not been addressed and (2) introducing theoretical concepts from sociology to study these topics with a focus on racial inequality. 

Speaking about the simulations of filters from HSLS, Revewer 1 notes

> "we don’t need the [empirical] research here to know the answer -- even before the analysis, the manuscript cites literature that shows that gaps in test taking and scores by student background characteristics which necessarily suggests that filters using those criteria would exclude certain groups." 

Given that racial inequality in student list products has never been studied before, the descriptive analyses of individual search filters are important for demonstrating basic relationships between search filters and racial composition. Although the expected findings can be inferred from prior literature, it is more powerful to demonstrate these findings empirically. Additionally, like Norris's [-@RN4786] analysis of Moody's city credit rating algorithm, our simple analyses of HSLS shows future education researchers that third-party products that have not been studied before can be studied by applying simple descriptive statistics to publicly available data.

With that said, we agree with the Editor and Reviewer 1 that the empirical analyses did not make a sufficiently substantial contribution! Although the Editor/Reviewer1 found analyses that filter on multiple search filters simultaneously to be more interesting, sample sizes preclude us from selecting many filters simultaneously.

The addition of RQ2 and RQ3 -- and the associated conceptual framework subsection on product utilization -- motivate analyses that make a stronger contribution. Revised RQ1 focuses on individual search filters and racial composition. 

RQ2 motivates analyses about which search filters did universities in our data collection actually select, yielding new insights about how student list products are utilized. 

RQ3 motivates analyses of the racial composition of targeted student list purchases -- motivated by the conceptual framework -- that filter on multiple search criteria. These analyses include simulations from HSLS data and also analyses of actual student list purchases made by public universities [KARINA -- ADD A FEW SENTENCES ABOUT WHAT ANALYSES RUN AND THE CONTRIBUTION]

3. Requesting overview of college admissions and recruitment

> **Editor comment**. Third, Reviewer 2 and 3 both seek more of an overview in the paper on college admission and recruitment broadly. We understand that this is a sociological analysis, but it does feel like somewhere in the introduction or literature review, it would be helpful to include some space devoted to an overall overview that cites interdisciplinary research on college recruitment. Given that EEPA covers all levels of education and all disciplines, it would be useful to ensure that readers who are not experts in this individual area can have strong grounding in how college recruitment works.

**Author response**. We agree with this recommendation, which is reflected in the revised introduction and the revised "Background and Literature Review" section. 

The revised introduction begins by providing statistics about racial inequality in college access, followed by Hoxby's conceptualization as the market for college access as a "two-sided matching problem" where students lack information about where they should enroll and colleges lack information about which students they should enroll. <!-- Hoxby said that a primary barrier to efficient matches was colleges' lack of information about the achievement/aptitude of students and that the SAT/ACT exam overcame this problem by allowing colleges to make apples-to-apples comparisons about prospective students from different places. --> Hoxby's conceptualization motivates the idea of student list products as a match-making intermediary connecting universities to prospective students.

The revised "Background and Literature Review" section begins by defining enrollment management and the enrollment funnel. The enrollment management profession views both recruitment and college admission as part of the broader process of trying to enroll students. The enrollment funnel identifies broad stages in the process of enrolling students (e.g., leads, inquiries, applicants, admits). In turn, buying names is the primary recruiting intervention for identifying leads from the broader pool of prospects. Therefore, we show where student lists fit in the broader process of enrolling students. Our focus on "enrollment management" implicitly pushes back on language in the decision letter that implies that student lists are part of the "college admissions process" (e.g., Editor states "... focus on an understudied part of the college admissions process", Reviewer 1 states "...biases related to parts of the admissions process I largely take as given"). This distinction may seem semantic, and we acknowledge that colleges generally recruit the sorts of students they would like to admit. However, the enrollment management profession conceives of buying names (to identify leads) and admissions as two distinct processes within a larger process.

Whereas the original literature review focused solely on scholarship on recruiting from sociology, the revised literature review substantively reviews scholarship from both economics and sociology. Interdisciplinary scholarship on recruiting tends to be aligned with either economics or sociology (e.g., our prior work on off-campus recruiting), so we hope that our revised literature review sufficiently captures interdisciplinary scholarship on recruiting.

4. Justify assertions with citations

> **Editor concern**. Fourth, we agree with Reviewer 1 that it would be helpful to include citations or peer-reviewed references to justify assertions throughout the paper. We can certainly see why some statements are likely to be true, but the paper needs to provide clear textual citations that provide theevidence behind the statements. As an example, across pages 5 and 6, there’s a statement about the test-optional movement being an existential crisis for the College Board. Yet, the College Board makes the majority of its funding from administering the AP program (and hasexpanded its revenue streams by contracting with states to provide the SAT as a high school exit exam). If the authors want to make thatargument, there needs to be a clear explanation of the evidence that drives the argument. Another example is that the authors repeatedly arguethat micro-targeting is used for harm but, as Reviewer 1 notes, there needs to be justification for this argument included in the paper (this tiesinto the next point on the conceptual framework). These types of statements can be found throughout the paper. Therefore, a thorough revisionof the entire paper attending to ensuring there’s justification for statements of fact would be beneficial.

**Author response**. Our read is that the concern is about failing to provide citations for arguably untrue assertions that are not based on published peer-reviewed research. We agree with this recommendation and apologize for this failure in our original manuscript. For the most part, we addressed this concern by removing the assertion from the manuscript (e.g., claims that test-optional movement is existential crisis for College Board, claim that universities that outsource list buys to consulting firms are less knowledgable about list buys). Claims such as these were not central to the manuscript and so removing them also helps to reduce manuscript length. Additionally, some claims we made in the original manuscript were based on things we observed in the process of data collection [EXAMPLE, EXAMPLE]. The revised manuscript no longer makes the claim, but rather says the issue should be the focus of future research. For claims that are central to the manuscript -- e.g., micro-targeting and harm -- we made sure that claims were based on theory and empirical evidence from related empirical contexts.

5. Reduce scope and length of conceptual framework

> **Editor concern**. Fifth, while well written, the conceptual framework seeks to cover several different areas of scholarship. In line with trimming some of the frontmatter of the paper to fit page limits and to provide a more coherent narrative, it would be useful if the authors selected one to two of the theoriescurrently described. For example, the authors might focus on critical data studies to examine how algorithms are constructed/trained and theways institutions might wish to use them for good but could ultimately create more harm. This shift would allow more space to provide theadditional justification for the authors’ arguments.

This Editor concern relates to a concern raised by Reviewer 1:

> **Reviewer 1 concern**. Instead, the manuscript turns to algorithmic customer selection tools. A lot of baggage is placed on the word “algorithm,” but I worry about this because its definition and application to the student list purchasing process is a bit of a moving target. Initially, algorithms are simply any computer executed code. This definition doesn’t seem all that analytically interesting, but then the manuscript layers on actuarial practices where we try to use past behavior to predict future behavior. This does seem problematic in the case of college admissions because if we assume students always behave as they have in the past, then clearly we can’t use these algorithms to include previously excluded groups. Where I gethung up is that I don’t see this level of prediction in the student list filters, but maybe it just needs to be made more explicit for me. I certainly see it, for example, in the College Board’s geodemographic segments that can filter on past college-going behaviors, but again we don’t observeeither that prediction or even those filters in this research.

Additionally, the Editor and Reviewer 2 raised concerns about scope conditions for the concept "structurally racist input":

> **Editor concern**. Arguments are put forward that “several academic filters and geographic filters are structurally racist inputs” yet there is no clear explanation of when an academic filter would be a structurally racist input and when one would not be. 

> **Reviewer 2 concern**. The conceptual framework is well-written and does a good job connecting search filters used by colleges to structurally racist inputs. One thing that I think would strengthen this section is to offer a more detailed discussion of how these inputs come to be racialized. For example, the authors note that zip codes are racialized but do not offer citations or a deeper discussion of this. Offering examples of policies and practices that have led zip codes to become a racist input would help strengthen the paper’s contribution. Similarly, the authors note factors that contribute to inequities in test scores by race, but this discussion, I think, is fairly surface level and doesn’t fully engage with the extent to which resources have been extended to some, primarily White schools, and denied to other schools in ways that create different educational opportunities. The authors start to do this (e.g., on page #), but I think a more detailed discussion would be helpful in establishing just the extent to which these inputs are likely to be racialized. I could see this being a great piece to cite in the broader admissions and financial aid literature (regarding racialized evaluation metrics), so more developed connections in the conceptual framework would be useful.

**Author response**. We agree that the conceptual framework covers too many different areas of scholarship (Editor), introduced/applied theoretical concepts that did not fit student list product characteristics being studied (Reviewer 1), and did not develop adequate scope conditions for the concept "structurally racist inputs" (Editor, Reviewer 2). Additionally, the decision letter (Editor, Reviewer 1, Reviewer 2) raised the idea that racial inequities in student list purchases are driven by universities utilizing student list products in service of admissions standards/priorities. Because empirical analyses should be motivated by the conceptual framework, the revised framework considers how colleges utilize student list product. The revised framework is the same length as the original framework. We believe this length is warranted given that the decision letter asked us to motivate analyses about product utilization, which was outside the scope of the origianl manuscript.

We rewrote the manuscript to address these concerns. The revised conceptual framework now addresses product utilization by colleges -- including admissions standards -- but reduces scope by focusing solely on scholarship/theory from sociology (the original manuscript aslo substantively included scholarship from data studies). The revised conceptual framework has three subsections: (1) **Selection Devices**, which introduces relevant ideas and concepts from sociology; (2) **Student List Products**, which applies these concepts to motivate analyses around RQ1 (propositions 1, 2, and 3); and **Utilizing Student List Products** which draws from the sociological literature on product utilization to motivate analyses for RQ2 and RQ3.

The original submission conceptual framework began with "algorithms" and "actuarial methods." Reviewer 1 very helpfully explained the flaws with this approach; defining algorithms "instructions written in code" is not analytically useful and the original manuscript did not analyze search filters that utilized actuarial methods! 

The revised manuscript begins by introducing the concept "selection devices" as processes or routines that allocate individuals to categories based on input factors. We then make the distinction between standardized selection devices -- in which the outcome is a mathematical function of the inputs -- and discretionary selection devices -- in which individual administrators exercise judgment about the outcome. Student list products are discretionary selection devices in which university administrators choose which search filters to filter on, which yields a set of prospects. Next, the Selection Devices section introduces the concept "racialized inputs" developed by @RN4786 which replaces the (homegrown) concept "structurally racist inputs" used in the original manuscript. @RN4786[p. 5] defines racialized inputs “those that are theoretically
and empirically correlated with historical racial disadvantage,” subjugation, and exclusion. I prefer Norris's concept because it is (newly) established in the sociology of race literature and it has scope conditions. 

Next, the Selection Devices section introduces "geographic inputs" and "predictive analytics inputs" as two kinds of racialized inputs studied in the sociology of race literature. Note that the revised manuscript analyzes student list purchases that utilize Geodemographic Segment search filters.

The "Student List Products" section motivates analyses for RQ1, which examines the relationship between individual student list product attributes and racial inequality, independent of how universities utilize student list products. Drawing from empirical research, we develop the arguments that test-score filters and geographic filters satisfy the @RN4786 criteria of racialized inputs. With respect to concerns raised by Reviewer 2, these arguments focus on the structural, historical roots in contemporary race-based residential segregation and race-based differences in test scores. 

The "Utilizing Student List products" section draws from sociological scholarship on product utilization to motivate analyses about how student list products were utilized by public universities in our data collection sample. The Editor and Reviewers 1 and 2 asked us to consider how student list purchases relate to admissions standards, which we do in this section in relationship to scholarship on product utilization vis-a-vis occupational/professional norms. However, we argue that it would be inappropriate for analyses of product utilization to stop there because the utilization literature identifies other important factors that drive how individuals in organizations use products. The literature raises the issue that racial inequality can arise when individuals utilize complex discretionary selection devices that interact with local patterns of segregation [@RN4795; @RN4801]. Additionally, the utilization literature argues that transparency and accountability are important guardrails against ascriptive bias in discretionary selection devices, but XXXX. REVISE TEXT additionally, in contrast to professions -- including admissions readers, there are no apparent rules about who may purchase student lists or training for how to purchase student lists. REVISE TEXT


6. Strengthen ties to policy

> **Editor concern**. Sixth, based on our read of the article, the ties to education policy need to be made clearer. We appreciate the discussion of implications onfederal regulations. To create a stronger connection with policy, we recommend the authors look at the feedback from Reviewer 1. We also noted that the discussion of policy relevance solely focuses on the federal government. Yet, these are all public institution student lists. It seems thatmore attention needs to be given to the role that state policy actors have in ensuring institutions are using these lists in an accountable way. This would likely necessitate a bit more attention to the fact that these institutions are public which likely means they also have higher acceptancerates than their private peers and different ways of marketing & evaluating applicants. For example, the last sentence on page 6 notes the privateschool counselors can do a lot to get students into “top colleges” by communicating that they will definitely enroll at that institution. Yet, this typeof demonstrated interest is frequently not used at public institutions. It would also be helpful to say more about whether states or the federal government have considered any policies or new regulations about these lists.

Related concerns by Reviewer 1

> **Reviewer 1 concern**. Finally, I’d strongly caution against the legal speculation here—it rests on quite a set of logical chains that I don’t think have much legal basis. I don’t think it is necessary for the manuscript to go in this direction.

**Author response**. We have made efforts to strengthen the ties to education policy and we have added policy implications for state policymakers. First, to create space we removed all detailed legal analysis/speculation. Additionally, as recommended by the Editor, we removed discussion of scholarship around private counselors.

The policy implications that the revised manuscript establish throughout the paper are the connection between student lists and consumer reporting products (e.g., FICO scores, products from Experian, Equifax). We lay the groundwork for this comparison immediately by beginning the manuscript with arguments from Hoxby [CITE] that standardized college entrance exams (SAT/ACT) were a primary driver in the emergence of a national higher education system because they enabled colleges to obtain information about prospective students that facilitated comparisons of prospects from different school systems. Later, College Board and ACT began selling student lists, which enabled colleges to transition from a model of accepting/rejecting applicants to a model of targeting desirable prospects. 
The conceptual framework builds this comparison by showing that the field of geodemography pioneered using geographic inputs alongside credit scores for the purpose of recruiting. We argue that this is similar to student list products utilizing geographic filters in concert with test scores. 

Finally, the implications section states that consumer reports are regulated by FRCA because they systematically lead to the extension of credit. We argue that student list products systematically lead to the extension of credit because buying names is the first intervention in the enrollment funnel and the final intervention is financial aid packages, including loans. The original manuscript made a similar assertion about student lists, but did not show substantive parallels between consumer reports and student lists. Additionally, the original manuscript veered into legal speculation (vis-a-vis FRCA XXX). the revised manuscript does not speculate, but rather says this is something that federal regulators should look into.

Interestingly, the field of geodemography pioneered using geographic inputs alongside credit scores in recruiting, which is similar to the using geographic filters in concert with test scores in 

7. representativeness of sample for geographic analyses

text

## Minor recommendations/concerns

# Reviewer 1

## Major recommendations/concerns

## Minor recommendations/concerns

# Reviewer 2

## Major recommendations/concerns

## Minor recommendations/concerns

# Reviewer 3

## Major recommendations/concerns

## Minor recommendations/concerns

@RN2247 credits the standardized college entrance exam for transforming U.S. higher education from a system of local autarkies to an efficient, national market by causing a 

