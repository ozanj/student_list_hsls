---
title: "Response to Editors and Reviewers"
author: "ozan"
date: "`r Sys.Date()`"
output: pdf_document
bibliography: ../../assets/bib/eepa_student_list.bib
csl: ../../assets/bib/apa.csl
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

We thank the Reviewers and the Editor for excellent, thoughtful, and detailed feedback. We were too "close" to the manuscript and could not see many of these flaws on our own. We agreed with the majority of concerns/suggestions and revised the manuscript accordingly. We disagreed with some concerns (discussed below). However, even these areas of disagreement, helped us identify areas where we had to make our arguments stronger and our explanations clearer. The result is a manuscript that we feel is dramatically improved. We are extremely grateful for the effort in care you have given to help improve this manuscript and to help us develop as scholars.

The revised manuscript is XX [KARINA] pages longer than the original. We feel this length is warranted by the additional issues that the Editor/Reviewers asked the manuscript to address.


# Editor 

## Major recommendations/concerns

1. What is the problem? student list products or college admissions priorities?

> **Editor concern**. *First, Reviewer 1 and 2 note that several of the arguments made in the paper are not necessarily about college lists, but instead about the admissions priorities of each institution. We believe it would be useful to tease this out more in the paper. We also believe it would be helpful to provide some (masked) information about the 14 institutions. This could be rounded enrollment numbers (in bands) or Carnegie Classification or something else. But as it feels like the argument is strengthened on why this might be about the admissions priorities of institutions, the reader would benefit from better understanding the contexts of the 14 institutions.*

> **Reviewer 1 concern**. *Where does the problem lie? The manuscript, in large part, seems inspired by the earlier FOIA request of student list purchases and the filters that determined the content of those requests. This is a cool FOIA request to think to make, and obviously a lot of work. The nature of the request made me think we’re dealing with how universities use student list requests either implicitly or explicitly to privilege certain groups in the search and recruitment process.*

**Author response**: We agree with these concerns raised by Reviewers 1 and 2. Our interpretation is, the problem is not just underlying architecture of student list products but how student lists are utilized by colleges (including college admissions priorities).

The revised manuscript re-framed the problem with student lists as (1) the underlying architecture of student list products and (2) how student list products are utilized by college administrators executing student list purchases. Alongside this re-framing, the revised manuscript is now motivated by three research questions:

- RQ1: What is relationship between individual search filters and racial composition of included vs excluded students? 
- RQ2: In what ways do public universities utilize racialized input search filters in concert with other search filters when purchasing student lists? 
- RQ3: What is racial composition of student list purchases that utilize racialized input search filters in concert with other search filters? 

RQ2 reflects the revised emphasis on how student list products are utilized in practice while RQ3 identifies racial composition that results from colleges utilizing student list products. The revised conceptual framework includes a subsection named "Utilizing Student List Products" that draws from the sociological literature on product utilization to inform targeted analyses of actual student lists purchased by public universities (this framework is discussed in more detail in a later response). Drawing from this utilization literature, college admissions standards/priorities are one factor that influences how colleges use student lists. However, our broader emphasis on "product utilization" is appropriate because focusing solely on admissions standards/priorities makes the implicit assumption that that racial inequality in student list purchases are primarily a function of decisions made with respect to admissions standards. This assumption is inconsistent with the literature on product utilization.

That said, the revised manuscript acknowledges that admissions standards are likely an important driver of student list purchases. Compared to public MA/doctoral universities, public research universities in our data collection sample were more likely to condition on test scores (as opposed to GPA) and were more likely to utilize higher test-score thresholds. We feel that these findings are sufficient to drive home the point that admissions standards matter. We do not provide detailed information about Carnegie Classification/selectivity/admissions standards as we did not feel that such information is essential given space limitations. Examining the relationship between college admissions priorities and student list purchases would require very granular analyses. For example, @RN4912 focus on racial/ethnic group at one college.

2. Scholarly contribution made by empirical analyses

> **Editor concern**. *Second, Reviewer 1 notes that the current analysis does not provide a substantial contribution to our understanding of admissions recruitment. The reviewer provides suggestions of ways the authors could expand the analysis to provide substantial insight into recruitment. We are not being prescriptive to say that the authors must follow Reviewer 1’s guidance. However, there needs to be more done. The current findings generally appear to align with what we already know about who takes college entrance exams or has higher academic GPAs. The current tests of proportions are not that revelatory given that the appendix tables show that almost every estimate is statistically significant (likely partially due to Type I error but also because these are large samples that will find practically insignificant differences statistically significant). More interesting are the findings that combining these elements creates a substantially more exclusive prospect pool (more could be done to explicitly tease this out). This also applies more specifically to the discussion section. Arguments are put forward that “several academic filters and geographic filters are structurally racist inputs” yet there is no clear explanation of when an academic filter would be a structurally racist input and when one would not be. For example, if an institution actually did use GPA and zipcode to expand the diversity of the prospect pool, is that a structurally racist input?If so, can structurally racist inputs be used for good? These are the more interesting, and challenging questions, that the paper could engage in.This would then also allow the authors to provide clearer policy recommendations.*

> **Reviewer 1 concern**. *The ultimate question appears to focus just on whether the filters themselves are structurally problematic. I think this may be a reasonable question, except that we don’t need the research here to know the answer—even before the analysis, the manuscript cites literature that shows that gaps in test taking and scores by student background characteristics which necessarily suggests that filters using those criteria would exclude certain groups. Is there doubt that these patterns have persisted since that research was produced?*

**Author response**. Consistent with the diversity of scholarship encouraged by the EEPA "Critical Approaches to Education Policy Research" call for proposals, the contribution this manuscript makes differs from a purely empirical contribution. We believe that the primary contributions of this research are: 

- Contribution 1: Motivate future critical education policy research on third-party products and vendors (which have not been the object of inquiry in prior research)
- Contribution 2: Introduce concepts from sociology that can be used to study these topics with a focus on racial inequality. 
  - The original manuscript fell short of this goal. We thank Reviewers 1 and 2 explaining this to us. We believe the revised framework is much stronger (discussed below).

We now discuss empirics. Racial inequality in student list products has never been studied before. Therefore, descriptive analyses of individual search filters are important for demonstrating basic relationships between search filters and racial composition. We concede that the broad, expected findings can be inferred from prior literature. However, isn't it more powerful to demonstrate these findings empirically with simple, clear graphs? 

Additionally, as part of Contribution 1, our analyses of HSLS show future education researchers that third-party products that have not been empirically analyzed before can be studied by applying simple descriptive statistics to publicly available data (In this way, we make a similar contribution to Norris's [-@RN4786] analysis of Moody's city credit rating algorithm).

With all that said, we agree with the Editor and Reviewer 1 that the empirical analyses did not make a sufficiently substantial contribution. Both the Editor and Reviewer 1 found analyses that filter on multiple search filters simultaneously to be more interesting. HSLS sample sizes limit analyses that would select on many filters simultaneously. Said differently, although 15,000 students is a very large student list purchase, 15,000 weighted HSLS respondents is a very small number of unweighted HSLS respondents.

However, the addition of RQ2 and RQ3 (alongside revised conceptual framework) motivate analyses that make a stronger empirical contribution

- RQ2 motivates analyses about which search filters did universities in our data collection actually select, yielding new insights about how student list products are utilized. 
- RQ3 motivates analyses of the racial composition of targeted student list purchases -- motivated by the conceptual framework -- that filter on multiple search criteria. 
  - These analyses include simulations from HSLS data and also analyses of actual student list purchases made by public universities 
  - Analyses based on HSLS09 assess the impact of combining academic and geographic racialized inputs together can have a compounding effect on racial disparities between included versus excluded prospects
  - Analyses based on actual student list purchases from our public records request data collection provide additional support for product utilization literature that suggests administrator discretion likely increases racial inequality (rather than minimizes it). Results suggest combinations of academic and geographic predictive analytics filters (Segment orders, Women in Stem orders) result in dramatic disparities in the number of Black and Hispanic prospects included in student list purchases.

3. Requesting overview of college admissions and recruitment

> **Editor comment**. *Third, Reviewer 2 and 3 both seek more of an overview in the paper on college admission and recruitment broadly. We understand that this is a sociological analysis, but it does feel like somewhere in the introduction or literature review, it would be helpful to include some space devoted to an overall overview that cites interdisciplinary research on college recruitment. Given that EEPA covers all levels of education and all disciplines, it would be useful to ensure that readers who are not experts in this individual area can have strong grounding in how college recruitment works.*

**Author response**. We agree with this recommendation, which is reflected in the revised introduction and the revised "Background and Literature Review" section. 

The revised introduction is substantially based on Hoxby's [-@RN2133;-@RN2247] conceptualization of the market for college access as a "two-sided matching problem" where students lack information about where they should enroll and colleges lack information about which students they should enroll. <!-- Hoxby said that a primary barrier to efficient matches was colleges' lack of information about the achievement/aptitude of students and that the SAT/ACT exam overcame this problem by allowing colleges to make apples-to-apples comparisons about prospective students from different places. --> This conceptualization motivates the idea of student list products as a match-making intermediary connecting universities to prospective students.

The revised "Background and Literature Review" section begins by defining enrollment management and the enrollment funnel. Enrollment management represents the organizational behavior side of college access. The enrollment management profession views both recruitment and college admission as part of the broader process of trying to enroll students. The enrollment funnel identifies broad stages in the process of enrolling students (e.g., leads, inquiries, applicants, admits). In turn, buying names is the primary recruiting intervention for identifying leads from the broader pool of prospects. Therefore, this subsection shows where student lists fit in the broader process of enrolling students. 

Whereas the original literature review focused solely on scholarship on recruiting from sociology, the revised literature review substantively reviews scholarship from both economics and sociology. Interdisciplinary scholarship on recruiting tends to be aligned with either economics or sociology (e.g., our prior work on off-campus recruiting), so we hope that our revised literature review succinctly captures the spectrum of scholarship on recruiting.

4. Justify assertions with citations

> **Editor concern**. *Fourth, we agree with Reviewer 1 that it would be helpful to include citations or peer-reviewed references to justify assertions throughout the paper. We can certainly see why some statements are likely to be true, but the paper needs to provide clear textual citations that provide the evidence behind the statements. As an example, across pages 5 and 6, there’s a statement about the test-optional movement being an existential crisis for the College Board. Yet, the College Board makes the majority of its funding from administering the AP program (and has expanded its revenue streams by contracting with states to provide the SAT as a high school exit exam). If the authors want to make that argument, there needs to be a clear explanation of the evidence that drives the argument. Another example is that the authors repeatedly argue that micro-targeting is used for harm but, as Reviewer 1 notes, there needs to be justification for this argument included in the paper (this ties into the next point on the conceptual framework). These types of statements can be found throughout the paper. Therefore, a thorough revision of the entire paper attending to ensuring there’s justification for statements of fact would be beneficial.*

**Author response**. We agree with this recommendation and apologize for this failure in our original manuscript. Our read is that the concern is about failing to provide citations for arguably untrue assertions that are not based on published peer-reviewed research. 

For the most part, we addressed this concern by removing the arguably untrue assertions that are not central to the manuscript (e.g., claims that test-optional movement is existential crisis for College Board, claim that universities that outsource list buys to consulting firms are less knowledgable about list buys). 

For claims that are central to the manuscript -- e.g., micro-targeting and harm -- we made sure that claims were motivated by theory and by empirical evidence from related empirical contexts.

5. Reduce scope and length of conceptual framework

> **Editor concern**. *Fifth, while well written, the conceptual framework seeks to cover several different areas of scholarship. In line with trimming some of the frontmatter of the paper to fit page limits and to provide a more coherent narrative, it would be useful if the authors selected one to two of the theories currently described. For example, the authors might focus on critical data studies to examine how algorithms are constructed/trained and the ways institutions might wish to use them for good but could ultimately create more harm. This shift would allow more space to provide the additional justification for the authors’ arguments.*

> This Editor concern relates to a concern raised by Reviewer 1:

> **Reviewer 1 concern**. *Instead, the manuscript turns to algorithmic customer selection tools. A lot of baggage is placed on the word “algorithm,” but I worry about this because its definition and application to the student list purchasing process is a bit of a moving target. Initially, algorithms are simply any computer executed code. This definition doesn’t seem all that analytically interesting, but then the manuscript layers on actuarial practices where we try to use past behavior to predict future behavior. This does seem problematic in the case of college admissions because if we assume students always behave as they have in the past, then clearly we can’t use these algorithms to include previously excluded groups. Where I get hung up is that I don’t see this level of prediction in the student list filters, but maybe it just needs to be made more explicit for me. I certainly see it, for example, in the College Board’s geodemographic segments that can filter on past college-going behaviors, but again we don’t observe either that prediction or even those filters in this research.*

Additionally, the Editor and Reviewer 2 raised concerns about scope conditions for the concept "structurally racist input":

> **Editor concern**. *Arguments are put forward that “several academic filters and geographic filters are structurally racist inputs” yet there is no clear explanation of when an academic filter would be a structurally racist input and when one would not be.*

> **Reviewer 2 concern**. *The conceptual framework is well-written and does a good job connecting search filters used by colleges to structurally racist inputs. One thing that I think would strengthen this section is to offer a more detailed discussion of how these inputs come to be racialized. For example, the authors note that zip codes are racialized but do not offer citations or a deeper discussion of this. Offering examples of policies and practices that have led zip codes to become a racist input would help strengthen the paper’s contribution. Similarly, the authors note factors that contribute to inequities in test scores by race, but this discussion, I think, is fairly surface level and doesn’t fully engage with the extent to which resources have been extended to some, primarily White schools, and denied to other schools in ways that create different educational opportunities. The authors start to do this (e.g., on page #), but I think a more detailed discussion would be helpful in establishing just the extent to which these inputs are likely to be racialized. I could see this being a great piece to cite in the broader admissions and financial aid literature (regarding racialized evaluation metrics), so more developed connections in the conceptual framework would be useful.*

**Author response**. We agree that the original conceptual framework covered too many different areas of scholarship (Editor), introduced theoretical concepts that do not fit the product characteristics being studied (Reviewer 1), and did not develop adequate scope conditions for the concept "structurally racist inputs" (Editor, Reviewer 2). 

Elsewhere in the letter, Editor/Reviewer 1/Reviewer 2 asked us to address the idea that racial inequities in student list purchases are driven by how colleges utilize student list products. We do so empirically. Therefore, the revised framework considers how colleges utilize student lists because empirical analyses should be conceptually motivated. The revised framework is a bit longer than the original framework. We believe this length is warranted because the decision letter asked us to take on a new area.

The revised conceptual framework has three subsections: 

- (1) **Selection Devices**, which introduces relevant ideas and concepts from sociology; 
- (2) **Student List Products**, which applies these concepts to motivate analyses around RQ1 (propositions 1, 2, and 3)
- (3) **Utilizing Student List Products** which draws from the sociological literature on product utilization to motivate analyses for RQ2 and RQ3.

Reviewer 1 very helpfully explained flaws with our original manuscript. We began by defining "algorithms" as "instructions written in code," but this definition is not analytically useful. The conceptual framework also began with a substantive treatment of actuarial methods, but the the original manuscript did not analyze search filters that utilized actuarial methods. 

The revised "Selection Devices" section begins by introducing selection devices as processes or routines that allocate individuals to categories based on input factors. We then make the distinction between standardized selection devices -- in which the outcome is a mathematical function of the inputs -- and discretionary selection devices -- in which individual administrators exercise judgment about the outcome. Student list products are discretionary selection devices in which university administrators choose which search filters to filter on, which yields a set of prospects. 

We introduce the concept "racialized inputs" developed by @RN4786[p. 5] as inputs “ that are theoretically and empirically correlated with historical racial disadvantage,” subjugation, and exclusion. Racialized inputs replace the (homegrown) concept "structurally racist inputs" from the original manuscript. We prefer Norris's [-@RN4786] concept because it is established in the literature and because it articulates scope conditions more clearly. We discuss "geographic inputs" and "predictive analytics inputs" as two kinds of racialized inputs studied in the sociology of race literature (Note: revised analyses include predictive analytics).

Second, The "Student List Products" section motivates analyses for RQ1, which examine the relationship between individual student list product attributes and racial inequality (independent of how universities utilize student list products). Reviewer 1 and Reviewer 2 raised concerns about the scope conditions for filters conceptualized as problematic. Drawing from empirical research and from concepts, we develop the arguments that test-score filters and geographic filters satisfy the @RN4786 criteria of racialized inputs. These arguments focus on the structural, historical roots in contemporary race-based residential segregation and race-based differences in test scores. 

Third, the "Utilizing Student List products" section draws from sociological scholarship on product utilization to conecptualize how colleges may utilize student lists. This section begins by reviewing findings about product utilization and racial inequality in discretionary selection process. Consistent with recommendations from Editor/Reviewer 1/Reviewer 2, this literature suggests that decisions about student list purchases are made with respect to college admissions priorities. 

However, the utilization literature identifies other important factors that determine racial inequality. In contrast to standardized selection devices, discretionary selection devices allow individual bias (implicit or explicit) to influence selection decisions. The utilization literature finds that both transparency and accountability are important guardrails against ascriptive bias in discretionary selection devices. We argue that these conditions are not present when colleges purchase student lists. Additionally, discretionary selection processes can yield unintended results because decision-makers have incomplete knowledge about the products they are using. The revised manuscript states that: "empirical research shows that Americans dramatically underestimate the magnitude of racial income inequality [@RN4884]. Discretionary selection devices that incorporate racialized inputs may produce racial inequality because decision-makers may have incomplete knowledge about how these inputs interact with local patterns of racial inequality [@RN4801; @RN4795]." Student list products are complex, powerful products that incorporate racialized inputs and are designed to facilitate micro-targeting by selecting multiple filters simultaneously. Therefore, we raise the possibility that racial inequality in student list purchases may be an unintended outcome.

6. Strengthen ties to policy

> **Editor concern**. *Sixth, based on our read of the article, the ties to education policy need to be made clearer. We appreciate the discussion of implications on federal regulations. To create a stronger connection with policy, we recommend the authors look at the feedback from Reviewer 1. We also noted that the discussion of policy relevance solely focuses on the federal government. Yet, these are all public institution student lists. It seems that more attention needs to be given to the role that state policy actors have in ensuring institutions are using these lists in an accountable way. This would likely necessitate a bit more attention to the fact that these institutions are public which likely means they also have higher acceptance rates than their private peers and different ways of marketing & evaluating applicants. For example, the last sentence on page 6 notes the privates chool counselors can do a lot to get students into “top colleges” by communicating that they will definitely enroll at that institution. Yet, this type of demonstrated interest is frequently not used at public institutions. It would also be helpful to say more about whether states or the federal government have considered any policies or new regulations about these lists.*

Related concerns by Reviewer 1

> **Reviewer 1 concern**. *Finally, I’d strongly caution against the legal speculation here—it rests on quite a set of logical chains that I don’t think have much legal basis. I don’t think it is necessary for the manuscript to go in this direction.*

> **Reviewer 1 concern**. *Products like the College Board’s search tool are blunt and imperfect and, as you demonstrate, likely problematic and exclusionary. But there aren’t many alternatives. What would an ideal tool look like that allows colleges to find students (affluent and otherwise) that have a reasonable likelihood of being admitted to the institution?*

**Author response**. We have made efforts to strengthen the ties to education policy and we have added policy implications for state policymakers. First, to create space we removed all detailed legal analysis/speculation. Additionally, as recommended by the Editor, we removed discussion of scholarship around private high school counselors.

With respect to federal policy, the revised manuscript has one federal policy implication that is developed throughout the paper. This is the similarity between connection between student lists products, which are unregulated, and consumer reporting products (e.g., FICO scores, products from Experian, Equifax), which are federally regulated. The revised Introduction lays the groundwork for this comparison as follows: Hoxby [-@RN2133;-@RN2247] argues that standardized college entrance exams (SAT/ACT) facilitated the emergence of a national higher education system because they enabled colleges to compare prospects from different school systems. Later, College Board and ACT began selling student lists, which enabled colleges to transition from a model of accepting/rejecting applicants to a model of targeting desirable prospects. The conceptual framework builds this comparison by showing that the field of geodemography pioneered using geographic inputs alongside consumer credit scores/reports for the purpose of recruiting. We argue that this is similar to student list products utilizing geographic filters in concert with test scores. 

Finally, the federal policy implications section of the Discussion states that consumer reports are regulated by FRCA because they systematically lead to the extension of credit. We argue that student list products systematically lead to the extension of credit via the enrollment funnel because buying names is the first intervention in a systematic process that culminates in financial aid packages (including loans) offered to admits. The original manuscript made a similar assertion about student lists, but did not show substantive parallels between consumer reports and student lists. Additionally, whereas the original manuscript veered into legal specultation, the revised manuscript does not.

Although not discussed in the manuscript, it may be useful for the Editor/Reviewers to know that we have had several conversations with lawyers who have expertise in consumer credit law. They have told us that student list products may satisfy the legal criteria for consumer reports. Further, they told us that this classification is the strongest and most realistic means of regulating student list products. The consumer protection lawyers we spoke with -- including lawyers at FTC who focus on education -- were unaware of the existence of student list products. Therefore, by making substantive comparisons between student lists and consumer reports, we make an important contribution to future federal policy debates.

The revised manuscript also discusses policy implications for the US Department of Education. In a "Dear Colleague" letter, @doe_dear_colleague_2023 signaled an intention to regulate third-party servicers involved in recruitment. This letter is most clearly targeted at online program managers. We recommend that the definition of third-party servicers be revised to include student list vendors.

The revised manuscript addresses the call for a discussion of state policy implications. We briefly describe an idea to create "public option" student list products based on statewide longitudinal student data systems: "Importantly, colleges would receive the contact information and academic achievement of students who opt in for free. Therefore, the public option would have no need for search filters that help colleges micro-target the "right" students. After obtaining the lists, the college could decide which prospects receive which recruiting interventions. @list_policy provide more detail about essential product features and challenges to overcome.

The revised manuscript argues that our proposed public option student lists complement the emerging state higher education policy trend towards "direct admissions." A recent post by SHEEO states that one barrier to equality of opportunity in direct admissions policies is reliance on third-party vendors student list vendors to identify eligble students [@sheeo_direct_lists]. Drawing from our "public option" proposal [@list_policy],  @sheeo_direct_lists recommend that states adopting direct admissions should develop student lists based on state longitudinal data systems. Indiana is currently developing a direct admissions policy. An associate commissioner for the Indiana Commission for Higher Education told us that they are using our proposal to develop student lists based on state administrative data.

7. Representativeness of sample for geographic analyses

> **Editor concern**. *Seventh, we have concerns about the geographic analysis. HSLS is a dataset that is representative at the national level and at the regional level. It is not representative at the city or zipcode level. That means that, even with weighting, all the geographic analysis is on a unique sample (that does aggregate up to being regionally and nationally representative but is not reflective of these smaller geographic zones). While we’re not saying these analyses must be removed, the authors need to seriously grapple with what the geographic analyses can provide evidence for. If the authors decide to leave this analysis in, there must be significant justification for its inclusion and discussion of just what these random datapoints can tell the reader about their underlying geographic areas.*

**Author response**. We thank the editor for bringing our attention to the ways in which we have misused the HSLS09 dataset. After consulting with a colleague who has expertise in the sampling procedures and extensive experience with using HSLS09, we believe our error was in using zip code as a "level of analysis." HSLS09 is designed to be representative at the national level for the U.S. population of high school students. HSLS09 is only representative at two lower geographies: at the state-level for a small subset of states and at the school-level for the base-year of the survey [(Ingels et al., 2015)](https://nces.ed.gov/pubs2015/2015036.pdf). Because HSLS09 sampling procedures do not include stratification at the metropolitan or county level, it is inappropriate to dissagregate the HSLS09 sample to these lower levels of geography and assume representativeness. Therefore, we removed analyes for Proposition 3 that applied a zip code filter to the sub-sample of HSLS09 students living in the Los Angeles and New York metropolitan areas. We have also dropped Proposition 4 that tested hypothesis about filtering on smaller geographic localities leading to greater racial disparities, which is also no longer theoretically well-motivated by changes in our conceptual framework noted above.

However, after much deliberation as a research team and in consultation with our colleague, we believe that analyses that use zip code as a "covariate" on the full, weighted HSLS09 sample do not violate assumptions regarding representativeness outlined above. After removing the problematic analyses noted above that used lower geographic localities as levels of analysis, we believe the remaining analyses kept in the revised manuscript that apply a zip code affluence filter based on percentile of affluence (which is motivated via previous literature/our conceptual framework and discussed in detail below) to the full weighted HSLS09 sample do not violate the issues outlined above for two reasons. 

First, by including the full weighted HSLS09 sample, our remaining analyses draw on the national representativeness of HSLS09 to presume that the weighted student sample includes students that live in zip codes across a range of income percentiles just the same as we rely on the HSLS09 sample to include students across a range of personal demographics (e.g., race/ethnicity, gender, first-generation college status, etc.) that is nationally representative. In the original submission, we dealt with variance in median household income varying widely across the U.S. by categorizing all U.S. zip codes by percentiles based on levels of income within CBSA. In order to ensure we are not inextricably using zip code as a level of analysis when categorizing zip code affluence, we dropped our within CBSA categorization (which did not include or categorize rural zip codes) and instead categorized percentiles based on levels within state. To test our presumption that HSLS09 weighted student sample includes students that live in zip codes across a range of income percentiles that is nationally representative, we calculated percentages of the total 313 Million 2012 U.S. population (in comparison to the 4.2 million weighted HSLS09 sample) based on zip code affluence levels.: 15.3% (16.8%) lived in zip codes within 0-19th percentiles of affluence, 17.8% (20.6%) lived in zip codes within 20-39th percentiles of affluence, 18.6% (16.5%) lived in zip codes within 40-59th percentiles of affluence, 21.3% (18.6%) lived in zip codes within 60-79th percentiles, 12.7% (12.4%) lived in zip codes within 80-89th percentiles, and 14.0% (14.2%) lived in zip categorized as greater than the 90th percentiles of affluence. We therefore conclude that it is reasonable to assume that the HSLS09 weighted sample is nationally representative of students living in zip codes across a range of income percentiles.

Second, our remaining analysis do not make inferences about zipcodes that rely or presume that HSLS09 students are representative of the zip code where they live. For example, it would be problematic to categorize a zip code as being a "high-achieving" zip code based on the academic credentials of a HSLS09 student living in that zip code. We do not categorize level of zip code affluence by HSLS09 students. Rather, we draw on U.S. Census data to categorize HSLS09 respondents on levels of affluence. Moreover, our goal to simulate the student list purchase process does not require students to be representative of their local communities. Our simulations using zip code affluence as an individual filter or in combination with other filters are reflective of the actual process of buying student lists-- students are either included or excluded based on the criteria specified by the college regardless of whether they are or are not representative of their local community.   

Based on our understanding of the problem being using zip code as a "level of analysis" and the appropriateness of using zip code as "covariate" via the full weighted HSLS09 sample, we did not drop the HSLS09 analysis of included versus excluded students resulting from a zip code filter across affluence percentiles (RQ1) and combinations of academic filters with zip code across affluence percentiles (RQ3). However, we are open to other/futher revisions if we have misunderstood the problem. 




## Minor recommendations/concerns

8. Confusing variable names

> **Editor concern**. *Eighth, given that the SAT and PSAT variables are actually measures of SAT/ACT (in SAT units) and PSAT/preACT (in PSAT units), it might behelpful to name those something like “college entrance exam” or “pre college exam” or something when talking about those thresholds. Ascurrently written, the paper can be confusing to the reader about what those variables actually mean.*

**Author response**. We agree that the SAT/PSAT variables names do not match how the measures are operationalized in HSLS09. We have followed the editor's advice and changed all text/figure references of "SAT" or "PSAT" to “college entrance exam” or “college pre-entrance exam," respectively.

9. Table notes

> **Editor concern**. *Ninth, Table 1 would benefit from a note that details the data (e.g., the fact that it covers multiple years, what a geomarket and segment is).*

**Author response**. This is an excellent suggestion. We have added a note to Table 1 that provides details about data collection and defines filters that are not commonly known.

10. Cut unnecessary text/topics to make room for topics requested by Editor/Reviewers

> **Editor concern**. *Finally, we recognize we have requested several spaces where the paper needs to engage more on topics. Given the page length concerns, it feels necessary for the literature review and discussion sections to be streamlined a bit more (the reviewers provide some suggestions for reorganization and trimming) and it also feels that Figure 1 (which is a reproduction of other scholars’ work), Figure 9, and Figure 10 could be cut or moved to the appendix. We agree with the reviewers that the authors may not have the legal expertise to provide some of the suggestions they do so it would likely make sense to remove the top paragraph on page 32. The first full paragraph on page 33 can likely be removed as well since it gets very detailed on certain parts of platform that are more ancillary to the larger point of the paper (it also includes a sentence that appears to be the same as a different sentence in the introduction). Several paragraphs on page 34 can likely be trimmed to a single sentence.These are just some suggestions for spaces but the entire manuscript should be evaluated to see where the narrative can be made tighter.*

**Author response**. We agree with these concerns and have made all requested cuts except one. The revised conceptual framework is not shorter because we added text about how colleges utilize student lists. Here is a list of cuts made from the original manuscript:

- Background section
  - removed subsection "The market for student list data" (about consultants and change in student list vendors)
- Conceptual framework
  - To reduce scope, removed substantive discussion of critical data studies
- Discussion section
  - Removed all detailed legal speculation
  - Removed paragraphs motivating broader critical education policy research focused on third-party products and vendor
    - including removal of detailed paragraph about Marx and data
  - removed the majority of discussion about enrollment management consulting firms
- Figures/Tables removed or moved to appendix
  - Removed Figure 1 (a reproduction of other scholars' work)
  - All proportional significance test tables moved to appendix
  - Combination of entrance exams and AP tests for RQ3 move to appendix

11. Define terms, reduce jargon

> **Editor concern**. *In recognition of the aim of this special issue to further conversation, understanding about, and attention to critical approaches to policy analysis in education, we are also requesting that all authors attend to a few additional points. Terminology varies across fields and terms common in one area may seem like “jargon” to others. Please take care to define key terms and unpack theories for this generalist audience. Provide detail on your data and methods, including explanation and justification of your analytic choices and processes (e.g. provide examples of codes),especially for qualitative and discourse methods that are less common in this journal. Address the implications of this work for policy and/or research. We hope that readers will use this special issue to reflect back on their own work, even if it is not in the same substantive domain of policy research. We believe that these measures will help strengthen the special issue and the role it can play in the broader field of education policy research; we appreciate your help in accomplishing these aims.*

**Author response**. We have taken steps to define terms and reduce jargon. In particular, the revised manuscript takes time to define "enrollment management." This term is substantively important because text in decision letter seemed to imply that student lists are "part of the college admissions process" (Editor, Reviewer 1). By contrast, the enrollment management profession conceives of recruitment and admissions as part of the broader process of enrolling students. Often, generalist readers are aware of (selective) admissions and are unaware of enrollment management. However, most colleges are non-selective, including the majority of colleges that buy lists. Therefore, it is important for readers to conceive of student lists as part of enrollment management -- which includes admissions -- rather than conceiving of lists solely through the lens of admissions.

# Reviewer 1

## Major recommendations/concerns

1. Where does the problem lie? Student list products or college admissions standards?

**Author response**: Our response to this reviewer concern can be found above

2. Third-party enrollment management consultants

> **Reviewer 1 concern**. This understanding [the problem is how colleges use student lists] of the purpose of the manuscript is dismissed a few times, however, in part by the assertion that universities, despite priorassumptions, do not do recruiting “individually,” but rather alongside third party enrollment management consultants. Here I wondered whetherthese third-party businesses would be the focus of the work, but these organizations largely stay on the periphery of the argument. In the end,I’m confused how much independence these businesses have from universities—don’t they act on behalf of the university’s stated enrollmentpreferences (which would still make me think that universities are independent when determining recruitment priorities)? Or is the idea that theyengage in student list filtering in a way that is more nuanced than what universities would do on their own and that universities may notunderstand the students who are excluded by these filters? Either way, we do not observe any actions by third-parties in this research (whichmakes me question whether they need as many words devoted to them as they do). The manuscript returns to third parties in the conclusion,specifically by citing that the US Department of Education thinks that the third party enrollment goals are determined by institutions—theimplication is that this may not be the case, but no evidence is presented to support this.

> **Reviewer 1 concern**. p. 5 I’m not sure I fully followed, or found much value in the timeline of search firms—a lot of the names, for example, won’t mean much to theaverage reader. I also had trouble following where the competition and concentration was in that paragraph. Revisit this section to see how much is necessary for the overall argument

We agree with Reviewer 1 that the emphasis on third-party consultants in the original manuscript was unhelpful and disorienting for the reader because third-party consultants were not addressed empirically in the manuscript. The revised manuscript removes all substantive text about enrollment management consulting until the discussion section. Here, we say that changes in the market for student list data (e.g., consultants becoming student list vendors and vice-versa) and the role of enrollment management consultants play purchasing names for colleges are topics that should be studied in future research.

3. Concerns about conceptual framework

**Author response**: Our response to this reviewer concern can be found above

4. Empirical analyses do not make a contribution

**Author response**: Our response to this reviewer concern can be found above

5. How are geographic filters used?

> **Reviewer 1 concern**. *How are geographic filters used? My impression here has always been that a university could say, “I want students from these states/counties/zip codes…[and then list them].” I imagine this option is available for public universities to target students in their home (or neighboring) state or within certain catchment areas (like the Cal State campuses have), or for private universities to recruit knowing that students tend to like attending college close to home. It would be helpful to confirm (and maybe you have) that selecting geographic areas by income characteristics is even an option (though I suppose it’s possible a university could find those zip codes on their own and then ask for them specifically in their list).*

> *Even if it is a common way of conducing geographic searches, I’m a little incredulous that a university would specifically search on geographic area income levels. I know that Author XXXX(d, e and f?) found that off-campus recruiting tended to focus on wealthier schools in large metropolitan and suburban neighborhoods, but I don’t think its safe to assume that schools would pursue the same recruitment strategy in both the student list development and their off-campus recruiting, or that this focus is the only recruitment priority of universities in their recruitment. As the manuscript notes, off-campus recruiting is often used to maintain relationships with certain high schools—something that being in person facilitates. Given the well-worn path to reaching students in these schools, it doesn’t make sense for universities to duplicate that effort when buying student lists. Maybe they do it in some cases, but it seems like a lot to base a whole section of analysis on speculation that colleges “may” do something.*

> *I reached out to my own admissions office and they use their student list purchases for two purposes. First, they use it to find first-generation, rural, and racially minoritized students they don’t find through other modes of outreach. Second, they use it to complete details about students (e.g., their high school) who they may have met at a college fair for whom they might only have a name and address. I know a former institution of mine similarly uses search to find students of color, and first-gen students. I’m not sure either of these admission offices would recognize the sort of searches you suggest with respect to looking for high income zip codes, though this is obviously based on anecdata with an n of 2. Either way, this part of the analysis feels unsubstantiated.*

**Author response**. We disagree with Reviewer 1 concerns. The fundamentals goal of this article is to investigate (a) the extent to which student list products can cause harm (defined as systematic racial exclusion) and also (b) whether they are likely to be used by colleges in ways that cause harm. This goal is qualitatively different than, say, trying to estimate the average treatment effect of an intervention. Rather, we believe that products that are capable of causing harm should be regulated. Therefore, the article identifies ways that student list products can be used that result in racial inequality. 

With respect to our statement that colleges "may filter on affluent zip codes when purchasing student lists," Reviewer 1 states: "Maybe they do it in some cases, but it seems like a lot to base a whole section of analysis on speculation that colleges “may” do something." We disagree with this statement. Speculation about what colleges "may" do with a product is exactly what should be done in an analysis of a product's potential for harm. Even if only one out of every ten colleges engage in this practice, the product is allowing behaviors that result in systematic racial inequality and this aspect of the product should be the focus of policy debate. 

Moreover, we suggest that targeting zip codes by affluence level is a very common practice. The reviewer is correct that universities can filter by zip code but not by the affluence level of zip code. The reviewer notes, "I suppose it’s possible a university could find those zip codes on their own and then ask for them specifically in their list." This is exactly when enrollment management professionals and consulting firms do. For instance, Ruffalo Noel Levitz offers a product that recommends how many names a university should buy from a zip code [@jmu_rnl]. We have revised the text in this paragraph to create a stronger rationale for analyses that examine racial composition by zip-code affluence, as follows:

> Prior scholarship on recruiting [e.g.,] [@RN3519; @RN4759; @RN4758; @RN4733] and articles in the popular press [e.g.,] [@RN4503; @RN4508; @RN4910] consistently find that selective private and public research universities disproportionately target affluent schools and communities. By contrast, for-profit colleges systematically target poor, communities of color [@cottom2017lower; @dache2018dangling]. Reporting by @RN4508 state that "consulting companies may estimate a student’s financial position by checking their Zip codes against U.S. Census data for estimated household incomes in that area." @RN4911 reported that "'Everybody wants to go to the magic island of full pay students, but it’s rapidly shrinking real estate,' said Bill Berg, an enrollment management consultant at Scannell & Kurz" and that "College Board does sell zip codes, which are a very good proxy for income levels, meaning colleges and their consultants could use the data to sort out rich and low-income kids." Collectively, these studies and articles suggest that selective private colleges and public flagship universities may filter on affluent zip codes when purchasing student lists, while for-profits may filter on low-income, urban zip codes. We expect that filtering for affluent neighborhoods is positively associated with racial exclusion because structures of racial segregation often prohibit people of color from living in affluent neighborhoods.

Reviewer 1 raises the issue that colleges may use student lists to target a different set of prospects than those targeted by off-campus recruiting visits. The rationale for our analyses do not rest on the assumption that colleges usually target buy names from the same communities they visit. Rather, the only necessary rationale for our geographic analyses is that they may do so. Anecdotally, for the University of Illinois-Urbana we have data on student list purchases and off-campus recruiting visits. The university makes substantially more out-of-state visits and name-buys than in-state. These out-of-state visits and name buys overlap substantially and focus on affluent, predominantly white schools and communities. These results are consistent with market research by consultancies, which recommend targeting prospects with multiple recruiting interventions [@RN4895; @RN4896]. This is an issue that we will address in a future manuscript.

Reviewer 1 reached out to their current and former institution and was told that student lists were used to primarily to identify underrepresented student populations. Based on on data we collected from public universities and based on reports by Ruffalo Noel Levitz {@RN4895}, this usage of student lists seems plausible, particularly at very selective private or public universities that enjoy strong enrollment demand. Even if some institutions use lists this way, our concern is that institutions may use lists in problematic ways because the product allows them to do so. Additionally, our analyses show that student list purchases designed to target underrepresented populations can reproduce other forms of inequality. Finally, we collected actual records of student list purchases with the help of four law firms. For several universities, their actual student list purchases differed substantially from what they said they used student lists for.

6. Comparing student list purchases to admissions thresholds

> **Reviewer 1 concern**. *When does the student list fail to create access? I certainly agree that structurally racist systems should not deny any student the opportunity to pursue a postsecondary degree. Thus, the fact that the filters that generate lists used in college recruitment include fundamental racial bias is a problem. However, I think it’s fair to say that the inclusion of, for example, academic filters in search criteria exist because of the centrality these academic factors have in admissions decisions themselves. Neither students nor universities benefit if a university put resources into recruiting students who are unlikely to be admitted to that university. Thus, it may be worth exploring whether student list generation is the problem, or the priorities of the admissions system in general. It is also possible that universities don’t cast as wide a net as they should—if you know the admissions statistics of the universities that you FOIA’s, as well as the thresholds those universities use in their search criteria, you could figure out whether universities were genuinely trying to capture all reasonable applicants (I’d hope for searching for students at their 25th SAT percentile or lower), or if they set their thresholds too high to be maximally inclusive.*

**Author response**. As stated earlier in this letter, we agree that the original manuscript did not adequately consider the relationship between student list purchases and admissions standards. The revised conceptual framework considers admissions standards and the revised analyses consider admissions standards, albeit in a broad, crude way. The results support the claim that student list purchases are related to admissions standards. The above comment from Reviewer 1 suggests more granular analyses between the student list purchases of a college and their admissions standards. For example, @RN4912 compared Harvard admissions standards to student list purchases of African American prospects. However, analyses such as these do not speak to the focus of our manuscript. Additionally, the granularity required would exceed the page limit of an already long manuscript (note that @RN4912 is about purchases by one college focusing on one sub-group of students). Therefore, the discussion section of the revised manuscript suggests that future research should be done in this area:

> Following @RN4912, future research should compare the student list purchases of an institution to their admissions standards. @RN4912 found that Harvard purchased names of African American prospects who were unlikely to be admitted. How widespread is "recruit to reject" and for which prospects? Another issue, do colleges filter on academic criteria (e.g., SAT/ACT scores) that are no longer admissions criteria?

7. What is the appropriate comparison group for included students?

> **Reviewer 1 comment**. This also makes me wonder whether “all other students” is the right comparison for included students. Regardless of their qualifications for selective admissions, not all students have an interest or desire in attending college. We obviously want to maximize the group who does what togo to college to attend, but I worry about whether you stack the deck too much in favor of your argument by including everyone in the excluded group descriptions. At the very least, I’m curious the extent to which students eligible to show up in a College Board search—they have a PSAT,SAT, or AP score—look like the population of students who enroll in college at all (I think it might be interesting here to think about both four-year college enrollment, and any postsecondary enrollment). If the college population is more diverse, it tells us that colleges are able to find more diverse students than they would if they relied on lists alone and that lists would fall short in helping universities identify appropriate applicants. If the list-eligible students are more diverse, it emphasizes the problematic elements of college recruiting in admissions processes. 

> Perhaps adding columns to the existing results that describes the composition of excluded students who ultimately enroll in a four-year college or any college at all would add strength to the analysis. I recognize the problem of assuming the demographics of the contemporary college-enrolled population as the “right” composition to strive for, but I suspect the list criteria would still have a hard time meeting this benchmark as a *minimum*.

**Author response**. We appreciate Reviewer 1's suggestions here, which raise important considerations regarding the appropriate comparison group. However, we believe that using the full weighted HSLS09 sample and simulating included/excluded groups is reflective of student list purchasing process, which is the aim of the current study. For example, Reviewer 1 suggests that regardless of qualifications for admissions, not all students want to go to college.  We don't believe that filtering our HSLS09 sample for college desire is appropriate to the aim of our study for various reasons. First, colleges cannot filter their student list purchases by whether or not a student has explicitly indicated a desire to attend college. Rather, by including questions regarding college *preferences* (e.g., college size, college major), the College Board questionnaire assumes all students completing the questionnaire via standardized assessments or college-search engines have a desire to attend college. Second, the process of assuming desire to attend college is corroborated by empirical literature that provides collective support that nearly all students aspire to college enrollment [e.g., @RN2011; @RN2012; @RN2013]. Moreover, even if some students do not desire to attend college, literature suggests that many of these students do not desire or aspire to attend college (or selective colleges) because of information asymmetries [@RN3699], lack of social capital necessary to navigate college-going [@RN1814], and are those in most need for early outreach interventions [@RN2014]. We argue that these students, which would be dropped from analysis altogether if we made a sampling decision based on desire/aspiration of college enrollment, would actually be those who could benefit the most from the targeted recruiting efforts that result from their profile being purchased by colleges and universities.    

We also believe that second recommendation by Review 1 to compare HSLS09 students included versus excluded across uses of filters to the population of those enrolled in college is beyond the scope of the current study. Given our goal is understand the extent to which student list products can cause harm (defined as systematic racial exclusion) and also whether they are likely to be used by colleges in ways that cause harm, the addition of analyses that compares HSLS09 students included/excluded in list purchases via filter combinations to those who ultimately enroll in college does not advance our study aims for two reasons. First, our goal is not to identify individual college's "good" or "bad" behavior as it relates to student list purchases. Moreover, given the historical racial inequity in college access (as outlined in revisions to the introduction), we don't believe that a college could be characterized as doing "good" if their student list purchases are relatively more diverse than their current student population or doing "bad" if they are less diverse. Rather, we believe the appropriate comparison group given the goals of the study is all potential students that could be included in student list purchases, which we believe we capture via the national representative HSLS09 sample.


## Other issues

- **Reviewer 1 concern**. *Do we have any sense of what percentage of postsecondary institutions buy student lists? Is it prevalent everywhere? Just four-year colleges?Just selective colleges?*

**Author response**. Because prior peer-reviewed scholarship cannot address this question, we are forced to rely on the "grey literature," particulary reports published by the consultancy Ruffalo Noel Levitz [e.g.] [@RN4402; @RN4895; @RN4896]. Reviewer 1's questions are addressed in the sub-section, "Background: Student List Products." The short answer is that student lists are for undergraduate recruiting are primarily purchased by selective and non-selective 4-yr public and private non-profit colleges.

- **Reviewer 1 concern**. *p. 1 How much weight should we put in the study about students opting in/out of the College Board’s search tool? Aren’t students taking prettyclear stances on their orientation to college enrollment here? From the description in the manuscript, it sounds like this is a descriptive study withsome pretty big omitted variable bias\ldots*

**Author response**. We appreciate Reviewer 1's prompting here, but we believe this line of questioning is also beyond the scope of the current study. Our central motivation for using the HSLS09 nationally representative sample is to estimate all potential prospective students that *could* be included in student list purchases to investigate the process of buying student lists and how likely the process is to create exclusion. Our study's aim is not to understand student behavior, which is what Reviewer 1 is inquiring about here (i.e., students who opt in to search tools are likely to highly motivated to go to college). If our aim was to understand student behavior, and moreover to understand a causal effect of opting in/opting out of College Board's search tool, then yes, we would likely be dealing with significant omitted variable bias. However, we believe that including all *potential* students that could be included/excluded in student list purchases via HSLS09 is appropriate to simulate how filter products can produce harm and how likely it is that filters could be used in ways that produce harm.

<!-- Omitted variable bias is a coefficient that is biased because of relationship with a variable excluded from the model. We are not attempting to estimate a causal effect here, but let's write out how this issue might play out: In our study, some students from HSLS who were included in our simulated purchases should have been in the excluded group because they opted out of CB/ACT search products. So we should be concerned if having this group in the included list rather than the excluded list substantively affects the racial composition of these lists. In particular, we should be concerned if white/asian students are more likely to opt out. if so, the composition of our included list is biased upwards with respect to white/asian students. -->


- **Reviewer 1 concern**. *I might work to refine the research question on p. 2., which asks “what is the relationship between student list search filters and the racial composition of students who are included versus excluded in student lists purchased from College Board?” I don’t think the data can answer this question (particularly since you don’t appear to have data on specifically how the filters are used in the requests from the College Board).*

**Author response**. The research questions have been revised, as described earlier in the letter. The revised research questions do not include the term "College Board"

- **Reviewer 1 concern**. *p. 4 I have a hard time parsing the final sentence before section 2.2. breaking it up in two sentences may be helpful. Also, if footnote 2 provides results for private institutions, what universities are included in the main text?*

**Author response**: This sentence refers to a report by @RN4402, which surveyed college about which means of initially contacting students (e.g. purchased lists, off-campus visit) results in desired recruiting outcomes (e.g., inquiries, application, enrollment). We feel that the original text is clear. Additionally, the main text clearly stated that the results were for public institutions. Given that data we collected were from public institutions, we put results for private institutions in the footnote to conserve space.

- **Reviewer 1 concern**. *p. 6. In the sentence about off campus visits and enrollment priorities, be clear about whose enrollment priorities. Also, is there another citation to use here? It’s a big claim and it would be good if someone other than you sees this link.*

**Author response**: We added @RN3519 to this sentence. The claim draws from sociological theories of organizational behavior (e.g., institutional theory, resource dependence theory) that basically argue that organizational resource allocation is a better indicator of organizational priorities what the organization says it cares about. Like the Joe Biden quote, "don't tell me what you value. show me your budget and I'll tell you what you value." In economics, a similar idea is revealed preferences. In the recruiting space, this claim is not unique to off-campus recruiting. Any recruiting intervention that requires resources should give an indication of enrollment priorities (although inferences must be made with caution). For example, @RN4904 make a similar claim in their analyses of advertising spending by postsecondary institutions.

- **Reviewer 1 concern**. *p.7 Is there a citation for the claim that universities are uninformed about which students they target? (last paragraph of the page)*

**Author response**: This is something we found in our attempts to collect data about student list purchases from 90 public universities. But we do not have a citation for this claim, so we have removed this claim entirely.

- **Reviewer 1 concern** *p. 11 In the first full paragraph, the manuscripts indicates that it is important to understand why “birds of a feather flock together” as the College Board states. It would be good to finish this thought and say why. If colleges are looking for diversity (which, maybe I naively think some of them are—at least a few of them seem to want to continue to use race conscious affirmative action, for example), and demographic sorting allows them to do it, explain why the cause of the sorting matters from the perspective of a diversity-seeking college. I’m not sure all readers will get there themselves. The manuscript returns to this idea on p. 15 and I’m not sure a reader would follow the logic any easier there.*

**Author response**: Our interpretation is that Reviewer 1 is talking about the potential conflict between college enrollment goals and equality of opportunity for students. For instance, a college that values racial diversity may achieve racial diversity by targeting students of color at private schools. Here, the enrollment goal of diversity is achieved but not equality of opportunity. We describe this issue in greater detail in @list_policy. We choose to not elaborate on this point in the present manuscript because of space limitations.

- **Reviewer 1 concern**. *p. 12 Be careful with your argument about the critique that “classifications systems are developed to optimize profit.” That may be true in thebusiness world, but I think many readers would argue that colleges don’t set out to optimize other things than (at least immediate) profit. Doesthat change the conclusions we draw from this claim?*

**Author response**: We agree with this concern. We have removed these claims from the revised manuscript.

- **Reviewer 1 concern**: *p. 17 Can you describe the 830 College Board orders in more depth? I’m still not sure what data you actually have here—is it what filters were used (broad categories) or what levels of those filters (e.g., SAT scores above 1200…). How do these data enter the analysis?*

**Author response**: We apologize for the confusion regarding the 830 College Board orders. We have added more detail in the Data sub-section of the Methods section that describes the associated data, which includes a link that shows an example of a College Board summary for a student list purchase.


- **Reviewer 1 concern**: *p. 18 Explain whether X3TGPAACAD is weighted or not and how this aligns with what students might report to the College Board.*

**Author response**: We have added text that clarifies X3TGPAACAD is an unweighted high school GPA in academic courses, which is a question asked in the pre-test questionnaire of College Board assessments. We have added a footnote that explains students can self-report their high school GPA on College Board's SAT Questionnaire by selecting from a 12-point interval scale ranging from 0.00 (F) to 4.33 (A+). However, we are unsure whether the questionnaire asks students to specify the weighting-scale used. Because schools use various weight-scales for reporting GPA and students are likely to report according to their school-calculated GPA, we use unweighted HSLS09 GPA as a conservative approach to capturing included students via minimum filter thresholds.


- **Reviewer 1 concern**: *p.23 Would the College Board have GPAs for anyone who didn’t take the PSAT, SAT, or an AP exam? Is that exercise in the analysis just a question of “who took any of those exams?”*

**Author response**: We understand the reviewers concern regarding only having GPA available in concert with test scores. For this reason and for space considerations, we have removed analysis of using a GPA filter individually.


- **Reviewer 1 concern**: *p.23 Don’t forget your CITE in the footnote.*

**Author response**: Done. 


- **Reviewer 1 concern**: *Throughout the results: I don’t know what p < 0.000 is as a significance level. Is this just a rounding/significant digit thing based on your statistical software? If so, report the lowest reasonable p-value that this rounding infers (I’m guessing p < 0.001)*

**Author response**: We apologize for the confusion. We have changed all instances of p<0.000 to p<0.001.  We have also added table notes to online appendices for statistical significance. 

- **Reviewer 1 concern**: *p. 30 (and other places) I think there’s a substantive difference between filtering by zip code (e.g., “we want students in these zip codes in the catchment area for CSU Long Beach”) and filtering on zip code affluence, but often filtering on affluence is referred to a simply filtering by zipcode, which I find confusing at best. Keep the distinction clear.*

**Author response**: We agree that this distinction is important. We have fixed instances of failing to make this distinction.

- **Reviewer 1 concern**: *How should we think about inclusion and exclusion based on SAT scores in a world where more states and localities are now requiring high schools to take the SAT/ACT?*

**Author response**: Given space limitations, the revised manuscript raises the issue -- and also test-optional movement -- as topics that should be studied in future research about student lists.

- **Reviewer 1 concern**: *How should a reader think about the timing of student list purchases? For example, purchases based on PSAT score allows universities to reach out to students much earlier in their college search process, potentially allowing them to shape not just the students’ preferences for college, but how they prepare. Purchases based on AP scores or SATs likely can only sway marginal preferences for where students apply.*

**Author response**: Reviewer brings up an excellent point here. @list_biz discuss timing of alternative sources of student list data in greater detail. We do not discuss these issues here because of space limitations.

- **Reviewer 1 concern**: *Products like the College Board’s search tool are blunt and imperfect and, as you demonstrate, likely problematic and exclusionary. But there aren’t many alternatives. What would an ideal tool look like that allows colleges to find students (affluent and otherwise) that have a reasonable likelihood of being admitted to the institution?*

**Author response**: Our response to this reviewer concern can be found above


# Reviewer 2


## Major recommendations/concerns

1. Describe recruiting and which colleges recruit, describe how/why student lists are important to which colleges

> **Reviewer 2 concern**. *The first is to step back a bit and provide a description of college recruitment more broadly: what colleges participate in recruiting? What is the motivation for colleges to recruit students and how does this differ depending on the type of college (e.g., public versus private, highly rejective versus open access)?*

> *Related, I think it would help to specify just how important student lists are to colleges. Perhaps this could be accomplished by clarifying what colleges use student lists (is this mostly highly rejectives or are regional comprehensives, community colleges, and other less rejective institutions using student lists to recruit?) – this helps identify the particular colleges that are more or less likely to engage in this process and the extent to which student lists are used in recruitment. Additionally, perhaps some of the authors’ prior work highlight just how important these lists are to colleges? For example, are they the primary way colleges identify a pool of students to recruit (versus looking at feeder high schools and prior year stats?)? I think it would help to discuss just how important these lists are or are not and to what institutions.*

**Author response**. We agree with these recommendations. The revised "Background and Literature Review" section of the manuscript implements these suggestions. First, the subsection "enrollment management and the enrollment funnel" defines the goal of enrollment management as controlling which students the college enrolls in order to realize enrollment goals. The enrollment funnel identifies stages and interventions in the process of recruiting students. This begins with buying names of prospects to identify "leads" and ends with offering financial aid to convert admits to enrolled students. Although this is implicit, all colleges engage in recruiting in order to realize enrolment goals. This is because all behavior along the enrollment funnel (buying names, visiting high schools, reading admissions files, offering financial aid) is part of the process of recruitment. 

Although this is stated succinctly earlier in the manuscript, the sub-section "Background: student list products" why colleges buy student lists are fundamentally important and which colleges buy student lists. Lists are fundamentally important (not just another recruiting intervention) because the names a college buys determines which prospects receive subsequent recruiting interventions designed to push them towards enrollment. Names that are not purchased are excluded from these subsequent recruiting interventions unless they reach out on their own. 

Leads are prospective students whose contact information has been obtained. Although identifying leads is important to nearly all colleges, not all colleges utilize lists as the primary means of identifying leads. As the revised manuscript states, Community colleges and institutions offering adult education programs often rely on behavioral-based leads (e.g., google display ads). @cottom2017lower shows that for-profits buy student lists, but not primarily from College Board/ACT because recent test-takers are not their target customers. Drawing from market research by @RN4895, the majority of public and private non-profit colleges buy names of high school students. Of the colleges that buy names, most buy more than 50,000 per year. Additionally, the revised manuscript states: Case studies and investigative reporting suggests that larger and more selective institutions purchase more names than smaller and less selective ones [@belkin2019-studata; @list_biz; @RN4912]. Even though selective/prestigious colleges receive many inquiries, they may buy names in order to increase applications (and lower acceptance rates) and/or to increase enrollment of underrepresented populations.


2. Sharpen conceptualization of structurally racist inputs

> **Reviewer 2 concern**. *The conceptual framework is well-written and does a good job connecting search filters used by colleges to structurally racist inputs. One thing that I think would strengthen this section is to offer a more detailed discussion of how these inputs come to be racialized. For example, the authors note that zip codes are racialized but do not offer citations or a deeper discussion of this. Offering examples of policies and practices that have led zip codes to become a racist input would help strengthen the paper’s contribution. Similarly, the authors note factors that contribute to inequities in test scores by race, but this discussion, I think, is fairly surface level and doesn’t fully engage with the extent to which resources have been extended to some, primarily White schools, and denied to other schools in ways that create different educational opportunities. The authors start to do this (e.g., on page #), but I think a more detailed discussion would be helpful in establishing just the extent to which theseinputs are likely to be racialized. I could see this being a great piece to cite in the broader admissions and financial aid literature (regarding racialized evaluation metrics), so more developed connections in the conceptual framework would be useful.*

**Author response**. We appreceate and agree with Reviewer 2 concerns here! Earlier in the letter, we explained how we refined our conceptualization of "racialized inputs" (which replaces "structurally racist inputs"), as  @RN4786[p. 5] "those that are theoretically and empirically correlated with historical racial disadvantage," subjugation, and exclusion. This conceptualization provides scope conditions. In turn, the revised manuscript can offer succint, but we think effective, arguments for why geographic filters (e.g., zip codes) and standardized test scores are racialized inputs. Geographic filters are racialized inputs because residential segregation is a function of historic structural racism in who can live where. Standardized test scores are racialized inputs because prior research shows -- theoretically and empirically -- that differences in standardized test scores by race are a function of racial segregation and disparities in education funding and education resources, which are a function of historic structural racism. 

3. Connection between student lists and broader inequities in admissions and education.

> **Reviewer 2 concern**. *One point that seems to be missing in the discussion is that the reason colleges select on filters in the first place is that they are often selecting on the same set of filters that they use to evaluate admissions applications. As a result, we could (and should, as the authors clearly show) change the way colleges recruit students, but absent additional overhauls to admissions criteria (and financial aid, college culture, etc.), the same outcomes are likely. Maybe some way to build this into the discussion is to highlight whether implications from this study for recruiting could similarly be used to rethink admissions criteria. I defer to the authors for how they want to address this, but it seems important to note that the entire system (K12 education, college admissions and aid) is designed to produce these inequitable outcomes, so changing recruitingpractices is one lever, but other levers similarly need to be pulled to produce different outcomes.*

**Author response**. We appreciate this recommendation! Earlier in the letter, we explained how the revised manuscript incorporates college admissions standards into the conceptual framework and analyses. 

Reviewer 2 argues that racial inequities in student list purchases are a function of racial bias in admissions standards. So changes in student list products will not cause change unless they are accompanied by changes in admissions policies. Furthermore, since the entire K-12 system is designed to produce racial inequality, changes to recruiting practices must be accompanied by broader structural changes in education in order to create real equality of opportunity. 

These are powerful, and provocative thoughts! At present, the revised manuscript does not substantively incorporate these ideas in the discussion section, even though we agree whole-heartedly. Why not? Our interpretation of the big-picture marching orders from Editor/Reviewers was that the manuscript was not sufficiently low to the ground in terms of focusing on college access, recruitment, student lists, and how colleges use them. Rather, the original manuscript got distracted by talking about changes in the enrollment management consulting industry, trying to make the case for a big-picture literature on "platform capitalism" in education, etc. So our revised manuscript tries to focus on being lower to the ground and going deeper on the big things that were asked for. In doing so, we lost some of the forest-level, big picture thinking. 

I do think we are at a moment of potential structural change -- both for good and for bad. Many colleges are going test-optional and to the extent that the test has been a racialized barrier, this is exciting. At the same time, segregation is getting worse, and there is great uncertainty about the future of student list products and recruiting more generally. At present, the revised manuscript does not address these issues, but we would be happy to take them on in a limited way in the discussion if the Editor/Reviewers would like us to do so.


## Minor recommendations/concerns

- **Reviewer 2 concern**: *The authors mention “platform studies in higher education” a few times in the manuscript – it’d be helpful to define this at first mention and discuss what some of these platforms are and the role they play for universities*

**Author response**: The revised manuscript has removed substantive discussion of platform studies due to scope and page limitations concerns. We briefly introduce this literature in the last paragraph of the manuscript, and we revised text to explain what digital platforms are and what this literature cares about.

- **Reviewer 2 concern**: *The second paragraph notes that student lists are important for degree completion – it wasn’t immediately clear to me what the connection here is, so making it explicit could help.*

**Author response**: Due to space limitations, we relegated findings about degree completion to a footnote, but we added a sentence to the end of the footnote explaining the potential mechanism between student lists and degree completion: "The logical mechanism is that where students start college affects the probability of graduation [@RN2261; @RN1494]"

- **Reviewer 2 concern**: *The first full paragraph on page 2 offers a lot of key definitions for the paper one after another – it’s easy to get a little lost in this paragraph, so maybe it would help to offer more transitions or descriptions of each key term and clarify how it connects to others*

**Author response**: We agree with this concern. The revised introduction has replaced this paragraph with text that introduces fewer concepts/definitions. 

- **Reviewer 2 concern**: *The end of the introduction methods unfair practices criteria under the FTC Act, but this is the first time it’s mentioned, so it would help to clarify what the authors mean by this and how it relates to the student lists (the authors are clear about it in the discussion, but the first mention doesn’t have much context)*

**Author response**: The revised manuscript removes legal speculation, as requested by Editor/Reviewers. The revised manuscript does not discuss or mention the FTC Act

- **Reviewer 2 concern**: *Is there a citation for Figure 1? Or does it draw on previous literature to identify the funnel?*

**Author response**: Yes, it is a commonly used conceptual model and we have added a couple of citations to Figure 1 that showcase this.

- **Reviewer 2 concern**: *The authors mention a period of vertical acquisitions in the enrollment management industry, but I’m not sure I saw a discussion of these vertical acquisitions in the text that follows*

**Author response**: Agreed. The revised manuscript has removed discussion of vertical acquisitions.

- **Reviewer 2 concern**: *Page 5 – authors mention ACT and College Board offer clients information about prospects that are not included in student lists – what type of information is this?*

**Author response**: In order to reduce word count and scope, the revised manuscript does not discuss ACT/College Board enrollment management consulting services in any depth.

- **Reviewer 2 concern**: *In section 3 that starts on page 6, the authors mention prior scholarship that focuses on the later stages of the enrollment funnel – perhaps cite some of this literature?*

**Author response**: We have added a few citations to give readers a sense of this literature.

- **Reviewer 2 concern**: *“Blind spot” on page 6 – consider changing the language to remove the word blind*

**Author response**: Thank you for this suggestion. We have removed this language.

- **Reviewer 2 concern**: *On page 8, the authors mention critical geography, but I think this is one of the first times it is mentioned in the manuscript – maybe mention this in the introduction and offer a brief description of how it informs the study*

**Author response**: In the interest of reducing scope, the revised manuscript develops a conceptual framework solely from sociology and has eliminated discussion of critical geography. 

- **Reviewer 2 concern**: *Page 9 – what is the connection between actuarial methods and selection devices? I think making this explicit would be helpful for the reader*

**Author response**: The revised manuscript makes this connection more explicit. We also replaces the term "actuarial methods" with "predictive analytics." 

We introduce "selection devices" as the first and primary concept of the conceptual framework (CF). Later in the CF, we introduce predictive analytics, which may be used as the outcome of a standardized selection device or maybe used as an input in a discretionary selection device. Student list search filters like "geodemographic segment" are examples of using predictive analytics as the input in a discretionary selection device.

- **Reviewer 2 concern**: *In section 4.2.1, the authors posit two broad sources of structural inequality – might be worth noting that these two sources are deeply connected – i.e., the input itself determines who is included/excluded in the underlying data while the search filters further exacerbate these differences inwho is included/excluded*

**Author response**: We agree with this recommendation. Due to space limitations, we have kept text noting this connection brief. The revised text reads: "The second source of structural inequality in student list products is the use of racialized inputs as search filters, which builds on differences in who is in the underlying database and exacerbates differences in who is included in student list purchases."

- **Reviewer 2 concern**: *Round sample sizes to nearest 10 for HLS09 data reporting throughout*

**Author response**: We appreciate this concern and careful read. Per NCES, we have rounded all unweighted sample sizes throughout the manuscript. Due the rounding of sample sizes to thousands in figures for space considerations, we believe unweighted samples are also in the clear for reporting regulations.

- **Reviewer 2 concern**: *In discussing the 2 data sources on pages 16-17, can you clarify which data are used to investigate each proposition?*

**Author response**: We apologize for the confusion. We have revised the Data sub-section to provide more details about data sources and which data sources are used for answering research questions 1-3.

- **Reviewer 2 concern**: *Are both Table 1 and Figure 3 necessary? It might be possible to just have one of these given the number of tables and figures*

**Author response**: We feel that both Table 1 and Figure 3 are necessary, particularly given the Editor/Reviewer request for analyses about how colleges utilize student lists. Figure 3 counts the number of times each filter was selected in the 830 student list purchases. But each student list purchase selects on multiple filters and Figure 3 does not convey this. Table 1 conveys which filters were most often specified together in student list purchases. However, in recognition of the number of tables/figures, we have dropped several tables/figures from the original manuscript. 

- **Reviewer 2 concern**: *In 6.3 starting on page 27, it might be useful to reference back to table 1 or figure 3 to note how these combined filters were selected (i.e., based on common search filters)*

**Author response**: We agree with Reviewer 2. We have referenced Table 1 in the revised section for _analyses based on HSLS09_ that explore common combinations of filters. 

- **Reviewer 2 concern**: *The authors note how much colleges pay per name, but it might help to provide an overall amount some colleges spend on student lists, just togive the reader a sense of the importance these lists play in recruitment*

**Author response**: The revised manuscript reports data from @RN4896 about the percentage of undergraduate recruitment budget allocated to student list purchases. However, we do not report actual amounts.


# Reviewer 3

## Major recommendations/concerns

1. Frame the manuscript vis-a-vis racial inequality in college access

> **Reviewer 3 concern**. *I would suggest the authors consider situating this study in a broader literature around college access. There is one paragraph in the beginning that discusses the probability of enrollment for students opting into the College Board Search Service, but I think there is more that could and should be said about existing racial inequities in college enrollment. What are current figures on the representation of students in public universities, for example? Have these numbers changed in recent years? Are they likely to worsen given affirmative action legislation or improve given the movement toward test-optional policies? In short, a brief discussion on the current representation of underrepresented groups in higher education helps to contextualize the broader impacts and implications of student lists and situate the magnitude of the problem that the study is illuminating. In essence, the so what? Likewise, the paper should end by coming back to this point about equitable access and opportunity to college for students from minoritized backgrounds.*

**Author response**. Thank you for this suggestion! We agree with Reviewer 3 that framing the manuscript vis-a-vis racial inequality in college access helpls situate the so-what of studying student lists. In the revised manuscript, the first paragraph focuses on racial inequality in whether and where students enroll in college. The introduction then shows that the emergence of student lists was fundamentally based on the standardized college entrance exam, which sociologists argue are mechanisms that amplify historic structural inequality. Additionally, we begin the discussion section re-stating the problem of racial inequality in college access. 

However, the manuscript does not go into great depth about racial inequality in college access (e.g., in relation to affirmative action legislation, test-optional policies). Our basic reason is that the manuscript is very long! So we make the assumption that the assumption that most readers already know that racial inequality in college access is an enduring problem. Considering the basic structure of the revised manuscript, if Reviewer 3 has ideas about how to succinctly weave scholarship/trends about racial inequality in access into this structure, we would be happy to consider those ideas.

2. Critical Quant Citations 

> **Reviewer 3 concern**. *The mention of micro-targeting and market segmentation made me think about critical quant studies in education policy research and the work of scholars (e.g., Samantha Viano and Dominique Baker) raising questions about the construction of large-scale datasets such as what are the types of variables that are being included. How are race and class conceptualized? How are other critical variables operationalized? It seems this literature is quite relevant to the paper and I would encourage the authors to consider featuring it more prominently or at least citing it in the discussion of critical data studies.*

**Author response**. We thank Reviewer 3 for this excellent suggestion. We have referenced the Viano and Baker 2020 study in the methods section under variables as we create a consolidated race/ethnicity via HSLS09 and College Board data that aligns with their recommendations regarding combining the measure of Hispanic ethnicity with the broader race measure. However, we are unable to incorporate into our discussion of critical data studies as the revisions to the manuscript have largely removed this section from our conceptual framework due to concerns regarding its length and scope. 


3. Unclear organization of the conceptual framework

> **Reviewer 3 concern**. *I was unclear/a bit confused by section 4.2 of the manuscript. It seems to repeat the preceding discussion of algorithmic selection devices. Is this the start of the conceptual framework? Should these mechanisms be collapsed in or incorporated into section 4.1 which discusses them as well(although using examples from outside of education)?*

> **Reviewer 3 concern**. *In section 4.2.1., there is a sentence, “Drawing from theory…” I would just add in theory for a better description. For example, sociological theory.*

**Author response**. We agree that the organization and flow of the original conceptual framework were convoluted and stilted. We believe that the revised manuscript has improved logical organization and flow, with the following sub-sections: (1) Selection Devices (introduce selection devices and what sociology knows about racial inequality in selection devices); (2) Student List Products (motivate analyses that examine racial inequality in underlying architecture of student list products); (3) Utilizing Student List Products (motivate analyses about how colleges utilize student list products)

4. Description of @RN4778

> **Reviewer 3 concern**. *The systematic exclusion of applicants of color is a key argument in the paper. As such, the authors might consider discussing the Hirschman and Bosk piece in more detail. It is cited throughout and seems to provide critical support for the authors’ claims. Is the current conceptual framework an extension of or a response to Hirschman and Bosk’s work?*

**Author response**. @RN4778 introduce the concept "selection devices" and offer other insights that manuscript relies on, but the conceptual framework is not fundamentally/solely based on this article. Therefore, we omit a longer description of @RN4778. 

5. Improve organization of discussion section and reduce redundancy

> **Reviewer 3 concern**. *I would encourage more organization of section 7, the discussion of the paper. At times, this section is a bit wandering and repetitive. Consider adding subsections for “Implications for Research” as well as subheadings to organize the various implications/future directions for research. And as stated above, I would circle back to the broader implications of this work for college access.*

**Author response**. We agree with these suggestions. The revised discussion section has the following organization and sub-headings

- Summary
- Policy Implications
  - Federal policy
  - State policy
- Implications for scholarship

Additionally, we worked to remove rendundancy from the discussion section and the section begins by framing issue importance vis-a-vis racial inequality in college access.

\newpage