---
title: "Response to Editors and Reviewers"
author: "ozan"
date: "`r Sys.Date()`"
output: pdf_document
bibliography: ../../assets/bib/eepa_student_list.bib
csl: ../../assets/bib/apa.csl
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Editor 

## Major recommendations/concerns

1. What is the problem? student list products or college admissions priorities?

> **Editor concern**. *First, Reviewer 1 and 2 note that several of the arguments made in the paper are not necessarily about college lists, but instead about the admissions priorities of each institution. We believe it would be useful to tease this out more in the paper. We also believe it would be helpful to provide some (masked) information about the 14 institutions. This could be rounded enrollment numbers (in bands) or Carnegie Classification or something else. But it feels like as the argument is strengthened on why this might be about the admissions priorities of institutions, the reader would benefit from better understanding the contexts of the 14 institutions.*

> **Reviewer 1 concern**. Where does the problem lie? The manuscript, in large part, seems inspired by the earlier FOIA request of student list purchases and the filters that determined the content of those requests. This is a cool FOIA request to think to make, and obviously a lot of work. The nature of the request made me think we’re dealing with how universities use student list requests either implicitly or explicitly to privilege certain groups in the search and recruitment process.

**Author response**: We agree with these concerns raised by Reviewers 1 and 2. We read Reviewer 1's concern as including admissions priorities, but being broader than admissions priorities (The first major concern of Reviewer 1 is "where does the problem lie" and Reviewer 1 states "The nature of the [public records] request made me think we're dealing with how universities use student requests either implicitly or explicitly to privilege certain groups in the search and recruitment process").

Thus, the revised manuscript reframed the problem with student lists as (1) the underlying architecture of student list products and (2) how student list products are utilized by college administrators executing student list purchases. Alongside this reframing, the revised manuscript is now motivated by three research questions:

- RQ1: What is relationship between individual search filters and racial composition of included vs excluded students? 
- RQ2: In what ways do public universities utilize racialized input search filters in concert with other search filters when purchasing student lists? 
- RQ3: What is racial composition of student list purchases that utilize racialized input search filters in concert with other search filters? 

RQ2 reflects the revised emphasis on how student list products are utilized in practice. The revised conceptual framework now concludes with a subsection named "Utilizing Student List Products" that draws from the literature on product utilization from sociology to inform targeted analyses of actual student lists purchased by public universities. This literature states that product utilization is partly a function of occupational/professional norms, which motivates the idea that criteria selected by individual colleges are likely to reflect college admissions standards. However, the utilization literature raises additional key factors that can yield racial inequality (e.g., implicit bias by individual decision-makers). In particular, we state that "Americans dramatically underestimate the magnitude of racial income inequality (Kraus, Onyeador, Daumeyer, Rucker, & Richeson, 2019). Therefore, discretionary selection devices that incorporate racialized inputs may produce racial inequality because decision-makers may be ignorant about how
these inputs interact with local patterns of racial inequality (Korver-Glenn, 2018)." These ideas raise the possibility that racial inequality in student list purchases may be the result of decision-makers having incomplete knowledge about how a complicated product (e.g., multiple intersecting fitlers) interacts with local racial segregation.

We believe that this broader emphasis on "product utilization" -- including admissions standards -- is preferable because focusing solely on admissions standards makes the implicit assumption that racial inequality in student list purchases are primarily a function of conscious decisions with respect to admissions standards, but this is inconsistent with findings from the literature on product utilization.

However, the revised manuscript acknowledges that admissions standards are likely an important driver of student list purchases. The revised manuscript includes results which show that public research universities in our data collection sample were more likely to condition on test scores (as opposed to GPA) and utilized higher test-score thresholds than public MA/doctoral universities. We feel that these findings are sufficient to drive home the point that admissions standards matter. We do not provide detailed information about Carnegie Classification/selectivity/admissions standards we did not feel that such information is essential given space limitations. However, this information could be added if Editors/Reviewers feel it is essential.

2. Empirical analyses do not make a contribution

> **Editor concern**. *Second, Reviewer 1 notes that the current analysis does not provide a substantial contribution to our understanding of admissions recruitment.The reviewer provides suggestions of ways the authors could expand the analysis to provide substantial insight into recruitment. We are not being prescriptive to say that the authors must follow Reviewer 1’s guidance. However, there needs to be more done. The current findings generally appear to align with what we already know about who takes college entrance exams or has higher academic GPAs. The current tests of proportions are not that revelatory given that the appendix tables show that almost every estimate is statistically significant (likely partially due to Type I error but also because these are large samples that will find practically insignificant differences statistically significant). More interesting are the findings that combining these elements creates a substantially more exclusive prospect pool (more could be done to explicitly tease this out). This also applies more specifically to the discussion section. Arguments are put forward that “several academic filters and geographic filters are structurally racist inputs” yet there is no clear explanation of when an academic filter would be a structurally racist input and when one would not be. For example, if an institution actually did use GPA and zipcode to expand the diversity of the prospect pool, is that a structurally racist input?If so, can structurally racist inputs be used for good? These are the more interesting, and challenging questions, that the paper could engage in.This would then also allow the authors to provide clearer policy recommendations.*

**Reviewer 1 concern**. *The ultimate question appears to focus just on whether the filters themselves are structurally problematic. I think this may be a reasonable question, except that we don’t need the research here to know the answer—even before the analysis, the manuscript cites literature that shows that gaps in test taking and scores by student background characteristics which necessarily suggests that filters using those criteria would exclude certain groups. Is there doubt that these patterns have persisted since that research was produced?*

**Author response**. We push back by arguing that this manuscript makes a contribution that differs from a purely empirical contribution. We believe that the primary contributions of this manuscript are not empirical, but rather (1) motivating future research on topics that have not been addressed and (2) introducing theoretical concepts from sociology to study these topics with a focus on racial inequality. 

Speaking about the simulations of filters from HSLS, Revewer 1 notes

> **Reviewer 1 concern**. *"we don’t need the [empirical] research here to know the answer -- even before the analysis, the manuscript cites literature that shows that gaps in test taking and scores by student background characteristics which necessarily suggests that filters using those criteria would exclude certain groups."*

Given that racial inequality in student list products has never been studied before, the descriptive analyses of individual search filters are important for demonstrating basic relationships between search filters and racial composition. Although the expected findings can be inferred from prior literature, it is more powerful to demonstrate these findings empirically. Additionally, like Norris's [-@RN4786] analysis of Moody's city credit rating algorithm, our simple analyses of HSLS shows future education researchers that third-party products that have not been studied before can be studied by applying simple descriptive statistics to publicly available data.

With that said, we agree with the Editor and Reviewer 1 that the empirical analyses did not make a sufficiently substantial contribution! Although the Editor/Reviewer1 found analyses that filter on multiple search filters simultaneously to be more interesting, sample sizes preclude us from selecting many filters simultaneously.

The addition of RQ2 and RQ3 -- and the associated conceptual framework subsection on product utilization -- motivate analyses that make a stronger contribution. Revised RQ1 focuses on individual search filters and racial composition. 

RQ2 motivates analyses about which search filters did universities in our data collection actually select, yielding new insights about how student list products are utilized. 

RQ3 motivates analyses of the racial composition of targeted student list purchases -- motivated by the conceptual framework -- that filter on multiple search criteria. These analyses include simulations from HSLS data and also analyses of actual student list purchases made by public universities [KARINA -- ADD A FEW SENTENCES ABOUT WHAT ANALYSES RUN AND THE CONTRIBUTION]

3. Requesting overview of college admissions and recruitment

> **Editor comment**. *Third, Reviewer 2 and 3 both seek more of an overview in the paper on college admission and recruitment broadly. We understand that this is a sociological analysis, but it does feel like somewhere in the introduction or literature review, it would be helpful to include some space devoted to an overall overview that cites interdisciplinary research on college recruitment. Given that EEPA covers all levels of education and all disciplines, it would be useful to ensure that readers who are not experts in this individual area can have strong grounding in how college recruitment works.*

**Author response**. We agree with this recommendation, which is reflected in the revised introduction and the revised "Background and Literature Review" section. 

The revised introduction begins by providing statistics about racial inequality in college access, followed by Hoxby's conceptualization as the market for college access as a "two-sided matching problem" where students lack information about where they should enroll and colleges lack information about which students they should enroll. <!-- Hoxby said that a primary barrier to efficient matches was colleges' lack of information about the achievement/aptitude of students and that the SAT/ACT exam overcame this problem by allowing colleges to make apples-to-apples comparisons about prospective students from different places. --> Hoxby's conceptualization motivates the idea of student list products as a match-making intermediary connecting universities to prospective students.

The revised "Background and Literature Review" section begins by defining enrollment management and the enrollment funnel. The enrollment management profession views both recruitment and college admission as part of the broader process of trying to enroll students. The enrollment funnel identifies broad stages in the process of enrolling students (e.g., leads, inquiries, applicants, admits). In turn, buying names is the primary recruiting intervention for identifying leads from the broader pool of prospects. Therefore, we show where student lists fit in the broader process of enrolling students. 

Whereas the original literature review focused solely on scholarship on recruiting from sociology, the revised literature review substantively reviews scholarship from both economics and sociology. Interdisciplinary scholarship on recruiting tends to be aligned with either economics or sociology (e.g., our prior work on off-campus recruiting), so we hope that our revised literature review sufficiently captures interdisciplinary scholarship on recruiting.

4. Justify assertions with citations

> **Editor concern**. *Fourth, we agree with Reviewer 1 that it would be helpful to include citations or peer-reviewed references to justify assertions throughout the paper. We can certainly see why some statements are likely to be true, but the paper needs to provide clear textual citations that provide theevidence behind the statements. As an example, across pages 5 and 6, there’s a statement about the test-optional movement being an existential crisis for the College Board. Yet, the College Board makes the majority of its funding from administering the AP program (and hasexpanded its revenue streams by contracting with states to provide the SAT as a high school exit exam). If the authors want to make thatargument, there needs to be a clear explanation of the evidence that drives the argument. Another example is that the authors repeatedly arguethat micro-targeting is used for harm but, as Reviewer 1 notes, there needs to be justification for this argument included in the paper (this tiesinto the next point on the conceptual framework). These types of statements can be found throughout the paper. Therefore, a thorough revisionof the entire paper attending to ensuring there’s justification for statements of fact would be beneficial.*

**Author response**. Our read is that the concern is about failing to provide citations for arguably untrue assertions that are not based on published peer-reviewed research. We agree with this recommendation and apologize for this failure in our original manuscript. For the most part, we addressed this concern by removing the assertion from the manuscript (e.g., claims that test-optional movement is existential crisis for College Board, claim that universities that outsource list buys to consulting firms are less knowledgable about list buys). Claims such as these were not central to the manuscript and so removing them also helps to reduce manuscript length. Additionally, some claims we made in the original manuscript were based on things we observed in the process of data collection [EXAMPLE, EXAMPLE]. The revised manuscript no longer makes the claim, but rather says the issue should be the focus of future research. For claims that are central to the manuscript -- e.g., micro-targeting and harm -- we made sure that claims were based on theory and empirical evidence from related empirical contexts.

5. Reduce scope and length of conceptual framework

> **Editor concern**. *Fifth, while well written, the conceptual framework seeks to cover several different areas of scholarship. In line with trimming some of the frontmatter of the paper to fit page limits and to provide a more coherent narrative, it would be useful if the authors selected one to two of the theoriescurrently described. For example, the authors might focus on critical data studies to examine how algorithms are constructed/trained and theways institutions might wish to use them for good but could ultimately create more harm. This shift would allow more space to provide theadditional justification for the authors’ arguments.*

This Editor concern relates to a concern raised by Reviewer 1:

> **Reviewer 1 concern**. *Instead, the manuscript turns to algorithmic customer selection tools. A lot of baggage is placed on the word “algorithm,” but I worry about this because its definition and application to the student list purchasing process is a bit of a moving target. Initially, algorithms are simply any computer executed code. This definition doesn’t seem all that analytically interesting, but then the manuscript layers on actuarial practices where we try to use past behavior to predict future behavior. This does seem problematic in the case of college admissions because if we assume students always behave as they have in the past, then clearly we can’t use these algorithms to include previously excluded groups. Where I gethung up is that I don’t see this level of prediction in the student list filters, but maybe it just needs to be made more explicit for me. I certainly see it, for example, in the College Board’s geodemographic segments that can filter on past college-going behaviors, but again we don’t observeeither that prediction or even those filters in this research.*

Additionally, the Editor and Reviewer 2 raised concerns about scope conditions for the concept "structurally racist input":

> **Editor concern**. *Arguments are put forward that “several academic filters and geographic filters are structurally racist inputs” yet there is no clear explanation of when an academic filter would be a structurally racist input and when one would not be.*

> **Reviewer 2 concern**. *The conceptual framework is well-written and does a good job connecting search filters used by colleges to structurally racist inputs. One thing that I think would strengthen this section is to offer a more detailed discussion of how these inputs come to be racialized. For example, the authors note that zip codes are racialized but do not offer citations or a deeper discussion of this. Offering examples of policies and practices that have led zip codes to become a racist input would help strengthen the paper’s contribution. Similarly, the authors note factors that contribute to inequities in test scores by race, but this discussion, I think, is fairly surface level and doesn’t fully engage with the extent to which resources have been extended to some, primarily White schools, and denied to other schools in ways that create different educational opportunities. The authors start to do this (e.g., on page #), but I think a more detailed discussion would be helpful in establishing just the extent to which these inputs are likely to be racialized. I could see this being a great piece to cite in the broader admissions and financial aid literature (regarding racialized evaluation metrics), so more developed connections in the conceptual framework would be useful.*

**Author response**. We agree that the conceptual framework covers too many different areas of scholarship (Editor), introduced/applied theoretical concepts that did not fit student list product characteristics being studied (Reviewer 1), and did not develop adequate scope conditions for the concept "structurally racist inputs" (Editor, Reviewer 2). Additionally, the decision letter (Editor, Reviewer 1, Reviewer 2) raised the idea that racial inequities in student list purchases are driven by universities utilizing student list products in service of admissions standards/priorities. Because empirical analyses should be motivated by the conceptual framework, the revised framework considers how colleges utilize student list product. The revised framework is the same length as the original framework. We believe this length is warranted given that the decision letter asked us to motivate analyses about product utilization, which was outside the scope of the origianl manuscript.

We rewrote the manuscript to address these concerns. The revised conceptual framework now addresses product utilization by colleges -- including admissions standards -- but reduces scope by focusing solely on scholarship/theory from sociology (the original manuscript aslo substantively included scholarship from data studies). The revised conceptual framework has three subsections: (1) **Selection Devices**, which introduces relevant ideas and concepts from sociology; (2) **Student List Products**, which applies these concepts to motivate analyses around RQ1 (propositions 1, 2, and 3); and **Utilizing Student List Products** which draws from the sociological literature on product utilization to motivate analyses for RQ2 and RQ3.

The original submission conceptual framework began with "algorithms" and "actuarial methods." Reviewer 1 very helpfully explained the flaws with this approach; defining algorithms "instructions written in code" is not analytically useful and the original manuscript did not analyze search filters that utilized actuarial methods! 

The revised manuscript begins by introducing the concept "selection devices" as processes or routines that allocate individuals to categories based on input factors. We then make the distinction between standardized selection devices -- in which the outcome is a mathematical function of the inputs -- and discretionary selection devices -- in which individual administrators exercise judgment about the outcome. Student list products are discretionary selection devices in which university administrators choose which search filters to filter on, which yields a set of prospects. Next, the Selection Devices section introduces the concept "racialized inputs" developed by @RN4786 which replaces the (homegrown) concept "structurally racist inputs" used in the original manuscript. @RN4786[p. 5] defines racialized inputs “those that are theoretically
and empirically correlated with historical racial disadvantage,” subjugation, and exclusion. I prefer Norris's concept because it is (newly) established in the sociology of race literature and it has scope conditions. 

Next, the Selection Devices section introduces "geographic inputs" and "predictive analytics inputs" as two kinds of racialized inputs studied in the sociology of race literature. Note that the revised manuscript analyzes student list purchases that utilize Geodemographic Segment search filters.

The "Student List Products" section motivates analyses for RQ1, which examines the relationship between individual student list product attributes and racial inequality, independent of how universities utilize student list products. Drawing from empirical research, we develop the arguments that test-score filters and geographic filters satisfy the @RN4786 criteria of racialized inputs. With respect to concerns raised by Reviewer 2, these arguments focus on the structural, historical roots in contemporary race-based residential segregation and race-based differences in test scores. 

The "Utilizing Student List products" section draws from sociological scholarship on product utilization to motivate analyses about how student list products were utilized by public universities in our data collection sample. The Editor and Reviewers 1 and 2 asked us to consider how student list purchases relate to admissions standards, which we do in this section in relationship to scholarship on product utilization vis-a-vis occupational/professional norms. However, we argue that it would be inappropriate for analyses of product utilization to stop there because the utilization literature identifies other important factors that drive how individuals in organizations use products. The literature raises the issue that racial inequality can arise when individuals utilize complex discretionary selection devices that interact with local patterns of segregation [@RN4795; @RN4801]. Additionally, the utilization literature argues that transparency and accountability are important guardrails against ascriptive bias in discretionary selection devices, but XXXX. REVISE TEXT additionally, in contrast to professions -- including admissions readers, there are no apparent rules about who may purchase student lists or training for how to purchase student lists. REVISE TEXT


6. Strengthen ties to policy

> **Editor concern**. *Sixth, based on our read of the article, the ties to education policy need to be made clearer. We appreciate the discussion of implications onfederal regulations. To create a stronger connection with policy, we recommend the authors look at the feedback from Reviewer 1. We also noted that the discussion of policy relevance solely focuses on the federal government. Yet, these are all public institution student lists. It seems thatmore attention needs to be given to the role that state policy actors have in ensuring institutions are using these lists in an accountable way. This would likely necessitate a bit more attention to the fact that these institutions are public which likely means they also have higher acceptancerates than their private peers and different ways of marketing & evaluating applicants. For example, the last sentence on page 6 notes the privateschool counselors can do a lot to get students into “top colleges” by communicating that they will definitely enroll at that institution. Yet, this typeof demonstrated interest is frequently not used at public institutions. It would also be helpful to say more about whether states or the federal government have considered any policies or new regulations about these lists.*

Related concerns by Reviewer 1

> **Reviewer 1 concern**. *Finally, I’d strongly caution against the legal speculation here—it rests on quite a set of logical chains that I don’t think have much legal basis. I don’t think it is necessary for the manuscript to go in this direction.*

**Author response**. We have made efforts to strengthen the ties to education policy and we have added policy implications for state policymakers. First, to create space we removed all detailed legal analysis/speculation. Additionally, as recommended by the Editor, we removed discussion of scholarship around private counselors.

The policy implications that the revised manuscript establish throughout the paper are the connection between student lists and consumer reporting products (e.g., FICO scores, products from Experian, Equifax). We lay the groundwork for this comparison immediately by beginning the manuscript with arguments from Hoxby [CITE] that standardized college entrance exams (SAT/ACT) were a primary driver in the emergence of a national higher education system because they enabled colleges to obtain information about prospective students that facilitated comparisons of prospects from different school systems. Later, College Board and ACT began selling student lists, which enabled colleges to transition from a model of accepting/rejecting applicants to a model of targeting desirable prospects. 
The conceptual framework builds this comparison by showing that the field of geodemography pioneered using geographic inputs alongside credit scores for the purpose of recruiting. We argue that this is similar to student list products utilizing geographic filters in concert with test scores. Finally, the implications section states that consumer reports are regulated by FRCA because they systematically lead to the extension of credit. We argue that student list products systematically lead to the extension of credit because buying names is the first intervention in the enrollment funnel and the final intervention is financial aid packages, including loans. The original manuscript made a similar assertion about student lists, but did not show substantive parallels between consumer reports and student lists. Additionally, while the original manuscript veered into legal speculation (vis-a-vis FRCA XXX), the revised manuscript does not. However, on background, we have had several conversations with legal experts who have told us that student list products may satisfy the legal criteria for consumer reports and that this classification is the strongest means of regulating student list products.

The revised manuscript considers policy implications around the US department of education signaling [CITE] in a "Dear Colleague" letter an intention to regulate third-party servicers involved in recruitment. This letter is most clearly targeted at online program managers. We recommend that the definition of third-party servicers be revised to include student list vendors.

The revised manuscript addresses the call for a discussion of state policy implications. We briefly describe an idea to create "public option" student list products based on statewide longitudinal student data systems: "Importantly, colleges would receive the “names” of students who opt in for free. Therefore, the public option would have no need for search filters that help colleges micro-target the "right" students. @list_policy provide more detail about essential product features and challenges to overcome."

Additionally, the revised manuscript argues that our proposed public option student lists complement the emerging state higher education policy trend towards "direct admissions." A recent post by SHEEO states that privately-led and state-led direct admissions policies to date rely on student lists purchased from third-party vendors [@@sheeo_direct_lists] and that this is a barrier to equality of opportunity in direct admissions. Draws from our "public option" proposal [@list_policy],  @sheeo_direct_lists recommend that states adopting direct admissions should develop student lists based on state longitudinal data systems. Currently, Indiana is in the process of developing a direct admissions policy and an associate commissioner for the Indiana Comission for Higher Education has told us that they are trying to implement our public option proposal.

7. representativeness of sample for geographic analyses

> **Editor concern**. *Seventh, we have concerns about the geographic analysis. HSLS is a dataset that is representative at the national level and at the regional level. It is not representative at the city or zipcode level. That means that, even with weighting, all the geographic analysis is on a unique sample (that does aggregate up to being regionally and nationally representative but is not reflective of these smaller geographic zones). While we’re not saying these analyses must be removed, the authors need to seriously grapple with what the geographic analyses can provide evidence for. If the authors decide to leave this analysis in, there must be significant justification for its inclusion and discussion of just what these random datapoints can tell the reader about their underlying geographic areas.*

**Author response**. KARINA ADDS TEXT

## Minor recommendations/concerns

8. Confusing variable names

> **Editor concern**. *Eighth, given that the SAT and PSAT variables are actually measures of SAT/ACT (in SAT units) and PSAT/preACT (in PSAT units), it might behelpful to name those something like “college entrance exam” or “pre college exam” or something when talking about those thresholds. Ascurrently written, the paper can be confusing to the reader about what those variables actually mean.*

**Author response**. KARINA - DO THIS AND THEN SAY THAT WE IMPLEMENTED THIS SUGGESTION.

9. Table notes

> **Editor concern**. *Ninth, Table 1 would benefit from a note that details the data (e.g., the fact that it covers multiple years, what a geomarket and segment is).*

**Author response**. KARINA - DO THIS AND THEN SAY THAT WE IMPLEMENTED THIS SUGGESTION.

10. Cut unnecessary text/topics to make room for topics requested by Editor/Reviewers

> **Editor concern**. *Finally, we recognize we have requested several spaces where the paper needs to engage more on topics. Given the page length concerns, itfeels necessary for the literature review and discussion sections to be streamlined a bit more (the reviewers provide some suggestions forreorganization and trimming) and it also feels that Figure 1 (which is a reproduction of other scholars’ work), Figure 9, and Figure 10 could be cutor moved to the appendix. We agree with the reviewers that the authors may not have the legal expertise to provide some of the suggestionsthey do so it would likely make sense to remove the top paragraph on page 32. The first full paragraph on page 33 can likely be removed as wellsince it gets very detailed on certain parts of platform that are more ancillary to the larger point of the paper (it also includes a sentence thatappears to be the same as a different sentence in the introduction). Several paragraphs on page 34 can likely be trimmed to a single sentence.These are just some suggestions for spaces but the entire manuscript should be evaluated to see where the narrative can be made tighter.*

**Author response**. SAY WHAT WE DID, BUT SAY THAT WE DIDN'T CUT CF CUZ THEY ASKED US TO INCORPORATE UTILIZATION.

11. Define terms, reduce jargon

> **Editor concern**. *In recognition of the aim of this special issue to further conversation, understanding about, and attention to critical approaches to policy analysis in education, we are also requesting that all authors attend to a few additional points. Terminology varies across fields and terms common in one area may seem like “jargon” to others. Please take care to define key terms and unpack theories for this generalist audience. Provide detail on your data and methods, including explanation and justification of your analytic choices and processes (e.g. provide examples of codes),especially for qualitative and discourse methods that are less common in this journal. Address the implications of this work for policy and/or research. We hope that readers will use this special issue to reflect back on their own work, even if it is not in the same substantive domain of policy research. We believe that these measures will help strengthen the special issue and the role it can play in the broader field of education policy research; we appreciate your help in accomplishing these aims.*

**Author response**. We have taken steps to define terms and reduce jargon. In particular, the revised manuscript takes time to define "enrollment management." This term is substantively important because text in decision letter seemed to imply that student lists are "part of the college admissions process" (Editor, Reviewer 1). By contrast, the enrollment management profession conceives of undergraduate recruitment -- which includes buying names to identify leads -- and college admission as distinct parts of the broader process of trying to enroll students. The majority of colleges are non-selective, which means that the majority of colleges that buy lists are non-selective. Therefore, we is important for readers to conceive of student lists as part of enrollment management -- which includes admissions -- rather than conceiving of lists solely through the lens of admissions.

# Reviewer 1

## Major recommendations/concerns

1. Where does the problem lie? Student list products or college admissions standards?

**Author response**: Our response to this reviewer concern can be found above

2. third-party enrollment management consultants

> **Reviewer 1 concern**. This understanding [the problem is how colleges use student lists] of the purpose of the manuscript is dismissed a few times, however, in part by the assertion that universities, despite priorassumptions, do not do recruiting “individually,” but rather alongside third party enrollment management consultants. Here I wondered whetherthese third-party businesses would be the focus of the work, but these organizations largely stay on the periphery of the argument. In the end,I’m confused how much independence these businesses have from universities—don’t they act on behalf of the university’s stated enrollmentpreferences (which would still make me think that universities are independent when determining recruitment priorities)? Or is the idea that theyengage in student list filtering in a way that is more nuanced than what universities would do on their own and that universities may notunderstand the students who are excluded by these filters? Either way, we do not observe any actions by third-parties in this research (whichmakes me question whether they need as many words devoted to them as they do). The manuscript returns to third parties in the conclusion,specifically by citing that the US Department of Education thinks that the third party enrollment goals are determined by institutions—theimplication is that this may not be the case, but no evidence is presented to support this.

> **Reviewer 1 concern**. p. 5 I’m not sure I fully followed, or found much value in the timeline of search firms—a lot of the names, for example, won’t mean much to theaverage reader. I also had trouble following where the competition and concentration was in that paragraph. Revisit this section to see how much is necessary for the overall argument

We agree with Reviewer 1 that the emphasis on third-party consultants in the original manuscript was unhelpful and disorienting for the reader since third-party consultants were not addressed empirically in the manuscript. The revised manuscript removes all substantive text about enrollment management consulting until the discussion section. Here, we say that changes in the market for student list data (e.g., consultants becoming student list vendors and vice-versa) and the role of enrollment management consultants play purchasing names for colleges are topics that should be studied in future research.

3. Concerns about conceptual framework

**Author response**: Our response to this reviewer concern can be found above

4. Empirical analyses do not make a contribution

**Author response**: Our response to this reviewer concern can be found above

5. How are geographic filters used?

> **Reviewer 1 concern**. How are geographic filters used? My impression here has always been that a university could say, “I want students from these states/counties/zip codes…[and then list them].” I imagine this option is available for public universities to target students in their home (or neighboring) state or within certain catchment areas (likethe Cal State campuses have), or for private universities to recruit knowing that students tend to like attending college close to home. It would behelpful to confirm (and maybe you have) that selecting geographic areas by income characteristics is even an option (though I suppose it’s possible a university could find those zip codes on their own and then ask for them specifically in their list).

> Even if it is a common way of conducing geographic searches, I’m a little incredulous that a university would specifically search on geographicarea income levels. I know that Author XXXX(d, e and f?) found that off-campus recruiting tended to focus on wealthier schools in largermetropolitan and suburban neighborhoods, but I don’t think its safe to assume that schools would pursue the same recruitment strategy in both the student list development and their off-campus recruiting, or that this focus is the only recruitment priority of universities in their recruitment. As the manuscript notes, off-campus recruiting is often used to maintain relationships with certain high schools—something that being in person facilitates. Given the well-worn path to reaching students in these schools, it doesn’t make sense for universities to duplicate that effort when buying student lists. Maybe they do it in some cases, but it seems like a lot to base a whole section of analysis on speculation that colleges “may” do something.

> I reached out to my own admissions office and they use their student list purchases for two purposes. First, they use it to find first-generation, rural, and racially minoritized students they don’t find through other modes of outreach. Second, they use it to complete details about students (e.g., their high school) who they may have met at a college fair for whom they might only have a name and address. I know a former institution of mine similarly uses search to find students of color, and first-gen students. I’m not sure either of these admission offices would recognize thesort of searches you suggest with respect to looking for high income zip codes, though this is obviously based on anecdata with an n of 2. Either way, this part of the analysis feels unsubstantiated.

**Author response**. We disagree with the Reviewer concerns. The fundamentals goal of this article is to investigate (a) the extent to which student list products can cause harm (defined as systematic racial exclusion) and also (b) whether they are likely to be used in ways that cause harm. Our goal is different than estimating the average treatment effect of a policy. Rather, we believe that products that are capable of causing harm should be regulated, so this article focuses on identifying ways that student list products can be used that result in racial inequality. With respect to our statement that colleges "may filter on affluent zip codes when purchasing student lists," Reviewer 1 states: "Maybe they do it in some cases, but it seems like a lot to base a whole section of analysis on speculation that colleges “may” do something." We disagree with this statement. Speculation about what colleges "may" do with a product is exactly what should be done in an analysis of a product's potential for harm. Even if only one out of every ten colleges engage in this practice, the product is allowing behaviors that result in systematic racial inequality and this aspect of the product should be the focus of policy debate. 

Moreover, we suggest that targeting zip codes by affluence level is a very common practice. The reviewer is correct that universities can filter by zip code but not by the affluence level of zip code. The reviewer notes, "I suppose it’s possible a university could find those zip codes on their own and then ask for them specifically in their list." This is exactly when enrollment management professionals and consulting firms do. For instance, Ruffalo Noel Levitz offers a product that recommends how many names a university should buy from a zip code [@jmu_rnl]. We have revised the text in this paragraph to create a stronger rationale for analyses that examine racial composition by zip-code affluence, as follows:

> Prior scholarship on recruiting [e.g.,] [@RN3519; @RN4759; @RN4758; @RN4733] and articles in the popular press [e.g.,] [@RN4503; @RN4508; @RN4910] consistently find that selective private and public research universities disproportionately target affluent schools and communities. By contrast, for-profit colleges systematically target poor, communities of color [@cottom2017lower; @dache2018dangling]. Reporting by @RN4508 state that "consulting companies may estimate a student’s financial position by checking their Zip codes against U.S. Census data for estimated household incomes in that area." @RN4911 reported that "'Everybody wants to go to the magic island of full pay students, but it’s rapidly shrinking real estate,' said Bill Berg, an enrollment management consultant at Scannell & Kurz" and that "College Board does sell zip codes, which are a very good proxy for income levels, meaning colleges and their consultants could use the data to sort out rich and low-income kids." Collectively, these studies and articles suggest that selective private colleges and public flagship universities may filter on affluent zip codes when purchasing student lists, while for-profits may filter on low-income, urban zip codes. We expect that filtering for affluent neighborhoods is positively associated with racial exclusion because structures of racial segregation often prohibit people of color from living in affluent neighborhoods.

NEXT UP - BITCH SLAP HIS N OF 2 ANALYSIS OF STUDENT LIST PURCHASES. FINE THAT THIS COLLEGE DOES IT THIS WAY, BUT STILL PROBLEMATIC THAT OTHER COLLEGES DO IT IN A DIFFERENT WAY. A DOWN THE ROAD QUESTION IS HOW TO PRESERVE BEHAVIOR THAT YOU OBSERVED IN ANECDATA WHILE PRECLUDING BEHAVIOR THAT RESULTS IN HARMFUL EXCLUSION. 

## Minor recommendations/concerns

# Reviewer 2

## Major recommendations/concerns

## Minor recommendations/concerns

# Reviewer 3

## Major recommendations/concerns

## Minor recommendations/concerns

@RN2247 credits the standardized college entrance exam for transforming U.S. higher education from a system of local autarkies to an efficient, national market by causing a 

