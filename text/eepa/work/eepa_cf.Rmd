---
title: "EEPA, CF"
subtitle: ""
author: 
  - Ozan Jaquette
  - Karina Salazar
bibliography: ../bib/eepa_student_list.bib
citeproc: no
output: 
  #bookdown::word_document2:    
  bookdown::pdf_document2:
    toc: FALSE
    pandoc_args: !expr rmdfiltr::add_wordcount_filter(rmdfiltr::add_citeproc_filter(args = NULL))
eoutput: pdf_document
always_allow_html: true
csl: ../bib/apa.csl
urlcolor: blue
fontsize: 12pt
#header-includes:
#      - \usepackage{pdflscape}
#      - \usepackage{geometry}
header-includes:
      - \usepackage{floatrow}
      - \floatsetup{capposition=top}
      - \usepackage{setspace}\onehalfspacing #\doublespacing
---


<!-- 


## Capital

_**Marx, Capital**_. Following, @RN4799 we conceptualize student list data as a form of capital. First, we introduce basic concepts. In _Capital_, @RN1423 describes the process by which capitalists invest money ($M$) to generate profit. A _commodity_ is an object that has both _use-value_ – the practical utility of an object – and _exchange-value_ – what the object can be exchanged for, usually measured in terms of money. The formula $C-M$ -- commodity for money -- represents a sale and the formula $M-C$ -- money for a commodity represents a purchase. 

@RN1423 distinguishes between two fundamental cycles between money and commodities. First, $C-M-C'$ is the formula for _consumption_; a commodity $C$ is sold for money $M$, which is then used to purchase another commodity $C'$. Here, the end goal is the use-value of commodity $C'$. The consumption transaction generates no profit -- because the exchange value of commodity $C$ is the same as the exchange-value of commodity $C'$ -- and at the end of this transaction the money $M$ ceases to be capital because it is expended.

Second, the formula $M-C-M'$ represents capital. Here, money $M$ is used to buy a commodity $C$, which is sold for money $M'$. In the formula for capital, $M'$ exceeds $M$, such that capitalists generate a "surplus value" or profit of $M' -M$. [@RN4799, p. 4] describes the "logic of capital accumulation

> this unending accumulation of capital, represented by $M-C-M'-C-M''-C-M''' \ldots$, is a defining feature of capitalism. \ldots, As Marx explains, ‘Use-values must therefore never be treated as the immediate aim of the capitalist; nor must the profit on any single transaction. His aim is rather the unceasing movement of profit-making’ (Marx, 1990: 254).


@RN1423 defined capital both as a process of generating profit and as an object utilized in the process of generating profit. @RN1423 differentiates between _money capital_ -- money invested to make more money -- and _real capital_. Real capital is differentiated into _constant capital_ -- the exchange value of input commodities (e.g., materials, machinery, and facilities) necessary to produce a commodity - and _variable capital_ -- wages paid to labor to produce a commodity.  


What is the source of surplus value for the capitalist? @RN1423 assumes that all commodities exchange at their exchange-value, meaning that the process of buying ($M-C$) and selling ($C-M$) are not the source of profit. Instead, the source of profit in the production of commodities is the exploitation of labor. 

Labor is paid a wage (exchange-value) that is less than the value (use-value) they add to the commodity. This trick is achieved by cloaking the reality of exploitation with the appearance of freedom. @RN1402 states that man owns his own labor and freely consents to exchange his labor for wages. By contrast, @RN1423 argues that man’s only means of subsistence is to sell his labor to capitalists. Therefore, the capitalist coerces labor to accept subsistence wages. Thus, surplus value (profit) is the difference between the (exchange?use?) value created by labor and the wages they are paid.

NOTE: Constant capital is purchased at its exchange-value.

_**Data as capital**_. Bulding on @RN1423, @RN4799 asks, what is the economic form of data? Conceptualizing economic capital as an object used to generate profit, @RN4799 conceives of data as form of capital that is distinct from economic capital, but has roots in economic capital. Furthermore, data can be converted into economic capital in two ways: as raw material (constant capital) and as a product (commodity) of digital labor. First, data can be conceptualized as "digital raw material" -- akin to lumber in construction -- "necessary in the production of commodities" [@RN4799, p. 4]. For example, software that predicts hospital staffing needs depends on historical data about patients. Second, @RN4799 conceptualizes data as a commodity that is 

> produced by the digital labour of people posting on Facebook, clicking on Google, exercising with Fitbits, and all the other things we do that create data and that data is created about. The cliche´ about the ‘free’ services provided digital platforms is that, ‘If you’re not the customer, you’re the product.’1 Through the work of using platforms and devices, people are turned into commodities that take the form of personal data, which is sold to advertisers and data brokers [@RN4799, p. 4].


Organizations approach data collection utilizing the logic of capital accumulation. @RN4799 p. 4 states that "the capitalist is not concerned with the immediate use of a data point or with any single collection, but rather the unceasing flow of data-creating." For example, @RN4799 p. 5 quotes an artificial intelligence researcher who said, "'At large companies, sometimes we launch products not for the revenue, but for the data. We actually do that quite often ... and we monetize the data through a different product'"

@RN4799 argues that data collection is better conceptualized as "data extraction" in order to highlight "the people targeted by, and the exploitative nature" (p. 6) of the process. Much of the data extracted by organizations are data about people (e.g., beliefs, behaviors, personal information). Data extraction typically occurs without "meaningful consent." In order to use a platform, users must sign "consent agreements" that entitle companies to extract, own, and sell data about its users.

Finally, data extraction typically occurs without fair compensation. Often, companies compensate users by allowing users to use the platform. In return, "the owner collects data as payment" [@RN4799, p. 8]. For companies, the exchange "value of data capital is massive. Some of the wealthiest companies in the world, like Facebook and Google, are built on data capital"  [@RN4799, p. 8]. Returning to @RN1423, capitalists generate profit by paying labor less than the value they add to the exchange value of commodities. In platform capitalism, the source of profit is paying users less than the value of the digital labor they provide.

_**Student list data as capital**_. We conceptualize student list data as economic capital, in the sense of an object used to generate profit. Consistent with @RN4799, nearly all student list data is derived from the user-data of students using some product. For example, College Board generates student list data from users of their testing services. Both "Free" college search engines (e.g., Cappex) and also college search software purchased by high schools (e.g., Naviance) created student list data from users looking for colleges and/or scholarships. As a commodity, student list data is a product of the labor of students. For example, student list data sold by College Board is created by the labor of students who fill out pre-test questionnaires about their college preferences and then complete the test. College search software (e.g., Cappex, Naviance) asks students to describe their college preferences and also extracts behavioral data from the efforts of students searching for college using the platform.

Because student list data are a commodity valued by universities looking for customers, the process of profiting from student list data follows follows cycles of money ($M$) and commodities ($C$) observed by @RN1423. A simple real-life  example of $M-C-M'$ is an organization (e.g., Cappex) invests money ($M$) to create a "free" college search engine ($C$) that generates student list data, which is then sold to universities for more money ($M'$). More lucrative approaches generate proprietary student list data as a by-product of money-making products, thereby following an $M-C-M'-C-M''$. For example, College Board invests money ($M$) to create standardized assessments ($C$), which is sold households $M'$ and also extracts student list data ($C$), which are utilized to create the Student Search Service product ($C$), which is sold to universities ($M''$) looking for students. The Naviance (college planning) and Intersect (college recruiting) software platforms -- now owned by PowerSchool -- follow a similar cycle. PowerSchool invests money ($M$) to purchase/maintain Naviance ($C$), which is sold to high school districts ($M'$) and also extracts student list data ($C$), which are utilized to create the Intersect recruiting platform  ($C$), which is sold to universities ($M''$) looking for students.


Finally, @list_biz observed that generating surplus value depends substantially on being an oligopolist supplier, that is extracting proprietary ownership over a large, unique pool of prospective students. For example, in 2011 the online textbook company Chegg tried to enter the student list business. Although Chegg extracted some student list data in-house -- through the acquisition of Zinch -- they were mostly a "lead aggregator," buying student list data from other venders ($M-C$) and reselling these data to universities ($M-C$). @RN1423 argues commodity exchange is not the source of surplus value because commodities are bought and sold at their market exchange-value. Consistent with this argument, the Chegg student list business failed to generate profit because they were paying market price for leads purcahsed from third-party suppliers [CITE].

Whereas vendors historically capitalized by selling student list data at a price-per-prospect to universities, organizations increasingly leverage market power in the supply of student list data to promote software produts and/or consulting services. The most common approach -- exemplified by the PowerSchool's Intersect product and EAB's Enroll360 product -- is to wrap a large pool of proprietary prospects within a software-as-service product designed to target these prospects. Universities that want to recruit these prospects must purchase the software. Similarly, College Board and ACT now leverage their databases of prospective students to promote enrollment management consulting services. Universities that purchases these consulting services gain access to information about prospective students that is not available within purchased student lists (e.g., GIVE EXAMPLE)

-->

# Theoretical Framework

## The Sociology of Race

### Structural Racism

Contemporary scholarship from the sociology of race is largely concerned with structural racism. This focus contrasts with "individualistic theories" -- common in psychology and economics -- "like implicit bias [that] tend to explain persistent racial inequality as a function of decisions made by (intendedly) nonracist actors in the context of formally equal, race-neutral practices" [@RN4778, p. 352]. Structural racism is “a form of systematic racial bias embedded in the ‘normal’ functions of laws and social relations” [@RN4760, p. 1143], whereby processes viewed as normal or neutral systematically advantage dominant groups and disadvantage marginalized groups [@RN4814]. This perspective views individual-level prejudices -- both explicit racism and implicit bias -- as "predictable outcomes of racialized systems rooted in white supremacy" [@RN4777]. Furthermore, the erosion of explicitly racist policies masks "colorblind racism," in which seemingly neutral policies ... [SEEMS TO REPEAT WHAT IS SAID IN ABOVE SENTENCE]

The sociology of race draws on historical scholarship from @RN4781;@RN4782 and @RN4773, which reveals the fundamental relationship between capitalism and race. 

@RN1423 describes capitalism as a system that emerges after feudalism whereby the source of profit is class exploitation of labor by owners. By contrast, @RN4773 argues that capitalism emerged during feudal Europe. Local elite constructed a hierarchical construct of race -- viewing Irish and immigrant ethnic groups as inferior -- in order to rationalize racialized division of labor. Thus, @RN4773 conceives of capiatlism as a system whereby the source of profit is exploitation based on the social construction of race.

Whereas historical scholarship from the "racial capitalism" tradition tends to focus on structural racism on the production side of the economy (labor), article focused on structural racism on consumer side of the economy, whereby people of color experience discrimination in credit market, housing markets, education, and other consumer markets.


### Algorithms, Actuarialism, and Micro-targeting

@RN4794 (215) define algorithms as "sets of instructions written as code and run on computers." Algorithmic products are attractive to investors because of the way these products scale. On the cost side, the marginal costs of adding more users is low. On the revenue side, each new user contributes new data to cull, improving the accuracy of the product and creating opportunities for new products.

Sociologists argue that algorithmic products utilize actuarial methods and are based on the logic of actuarialism.
are based on actuarial methods and actuarial logic [@RN4778;@RN4794; @RN4835]. Actuarial methods, pioneered by the insurance industry, proceed in two steps. First, apply statistical techniques to previous cases in order to identify factors positively and negatively associated with an outcome of interest. Second, apply these results to future cases in order to make predictions and to assign levels of risk to each case. Actuarialism is the ideology that equates fairness with risk, as determined by predicted probabilities. Under the logic of actuarialism, individuals or businesses that have characteristics associated with loan default should be charged higher interest rates.

@RN4794 observes that the adoption of actuarial methods across many industries was buoyed by concerns about racial equity following antidiscrimination legislation in the 1970s. Amidst mounting evidence about the explicit and (more recently) implicit biases of individual decision-makers, actuarial products removed individual discretion and promised standardized, "procedural fairness" based on objective data. @RN4778 argues that actuarial methods can promote racial equity when the prejudice of individual decision-makers is the source of racial inequality. For example, the analysis of residential real estate exchange by @RN4801 found that homes in predominantly white neighborhoods received higher appraisal values than those in non-white neighborhoods because appraisers have discretion in selecting similar comparison homes ("comps") for the valuation. When appraising homes in predominantly white neighborhoods, appraisers tended to restrict "comps" other white neighborhoods -- regardless of geographic proximity to non-white neighborhoods, whereas homes from non-white neighborhoods were appraised against homes in non-white neighborhoods.

However, scholarship from the sociology of race argues that actuarial methods often reinforce structural racism. We highlight two mechanisms. @RN4810 discuss "classification situations," defined as the use of actuarial techniques by organizations to categorize consumers into different groups. Historically, classifications were binary; consumers with "good" credit were offered loans and consumers with bad credit were not. Advances in data analytics (e.g., the calculation of individual credit scores) and the profit imperative led finer classifications. @RN4810 (p. 566) offer a quote from a banking trade publication: "Stop trying to lend at low margin to accountants, lawyers and civil servants who are reliable but earn the bank peanuts. Instead, find the customers who used to be turned away; by using modern techniques, in credit scoring and securitization, they can be transformed into profitable business." Thus, the emergence of classifications that categorize consumers into many groups, or along a continuum, is associated with the emergence of tiered products targeting different consumer groups with different benefit levels. For example, the "payday" loan industries targets consumers groups that were previously denied credit altogether, but charges excessive interest rates. @RN4774 defines "predatory inclusion" as the inclusion of "including marginalized consumer-citizens into ostensibly democratizing mobility schemes on extractive terms." Predatory inclusion is exemplified by the for-profit college industry, which systematically targeted women of color and generated profit by encouraging these students to take on loans [@cottom2017lower]. Other examples include sub-prime mortgage schemes targeted at minorities and the "gig economy" @RN4774.

A second source of structural racism in actuarial methods -- central to student list products -- is the use of structurally racist inputs. Actuarial products predict future outcomes by modeling the determinants of the outcome using historical data. Even when these models do not explicitly include race, they often include inputs that are correlated with race and that minorities tend to score poorly on because they have been historically excluded from this input. For example, @doi1011 found that a commercial algorithm used by hospital systems under-predicted the health care needs of Black patients because the algorithm used health care costs as a proxy for health needs, but Black patients tend to receive less care relative to their health needs than other patients. Thus, @RN4794 (p. 224) state that "predicting the future on the basis of the past threatens to reify and reproduce existing inequalities of treatment by institutions." Within the College Board suite of student list products, "geodemographic segment" filters classify each high school and neighborhood based on past college-going behaviors. The classification -- based on cluster analysis -- is highly correlated with race because communities of color that have been historically excluded from higher education are more likely to be lumped together.

@RN4786 offers an insightful analysis of structural racism in credit ratings of city governments. Ratings agencies (e.g., Moody's) utilize algorithmic methods to assign city governments a credit score based on determinants thought to predict the probability of loan default. When cities issue bonds, these credit scores affect the interest rates of the bond. @RN4786 models Moody's city credit rating score as a function of the criteria included in Moody's rating system. @RN4786 introduces the concept "racialized input" to describe the use of inputs that appear race-neutral, but actually disadvantage people of color due to historical exclusion. Furthermore, the inclusion of racialized inputs masks structural racism within within actuarial systems because racialized inputs "explain away" the relationship between race and the outcome. The use of "median household income" by Moody's rating system is a racialized input because earning differentials are a function of historical discrimination. When @RN4786 excludes household income from the model, having a larger share of Black residents is negatively associated with city credit rating. When analyses include this racialized input in the model, share of Black residents is no longer associated with city credit rating, thereby masking structural racism within city credit ratings

Scholarship within the tradition of critical data studies has observed the growth of micro-targeting and market segmentation in algorithmic products that target prospective customers [e.g., @RN4775; @RN4772; @RN4795; @RN4789]. Micro-targeting approaches promise to reach granular segments of society with great precision. For example, @RN4795 state that "Facebook microtargeting is driven not by a goal of making all users available to advertisers, but of making the 'right' individuals available" and that "Facebook advises that advertisers 'Implement a targeting strategy that focuses on reach and precision and eliminates waste.'" Student list products in the digital era develop filters that increase precision in targeting desired prospects and market themselves on micro-targeting. College Board Student Search promises to “create a real pipeline of best-fit prospects” @RN1623 while ACT Encoura uses the tag-line “find and engage your best-fit students” @RN1624. Enrollment management consulting firms promise precision when marketing list buying services. For example, Ruffalo Noel Levitz states the “RNL Student Search and Engagement” product enables universities to “target the right students in the right markets” by making “the most efficient name purchases using predictive modeling” [@ruffalo_noel_levitz_2021].


Scholars of critical data argue that racial exclusion is a predictable consequence of micro-targeting and market segmentation [@RN4775; @RN4772; @RN4795]. The process of developing a classification system requires software developers and companies to make a series of inherently subjective decisions (e.g., who is in the dataset, which measures to utilize, which categories to identify), creating opportunities for individual biases of developers and structurally racist inputs to enter the algorithm. @RN4795 (p. 3) argues that most classification systems are developed to optimize profit and "audiences are treated as a commodity to be bought and sold. When audience segments are under-valued in the market, demand among advertisers for the ability to reach them will be relatively low, which decreases the likelihood that a corresponding segment will be produced." A theme from scholarship on micro-targeting in politics is that these technologies *could* be used to to increase outreach to marginalized groups, but in practice they are not. Rather, micro-targeting practices raise concerns about "political redlining," whereby "Campaigns routinely 'redline' the electorate, ignoring individuals they model as unlikely to voite, such as unegistered, uneducated, and poor voters (Kreiss, 2012, p. X)[QUOTED FROM COTTER 2021].

### Conceptualizing student-list products

We conceptualize student list products vis-a-vis scholarship from the sociology of race and critical data studies. Scholarship from the sociology of race argues that the emphasis on explicit and implicit bias of individual decision-makers masks structural racism. Algorithmic products -- particularly products that generate a "score" for each customer -- reproduce racial inequality when these products utilize structurally racist inputs<!-- that systematically disadvantage communities of color because of historical exclusion from this input -->. Consistent with these ideas, student list products offer search filters that enable universities to target prospective students. Several search filters can be conceptualized as structurally racist inputs, for example zip code, AP score, and PSAT/AP score. While products (e.g., city credit ratings) that that utilize structurally racist inputs and lead to a numeric score are deterministically discriminatory, users of student list products may elect to not use structurally racist search filters -- or use these filters thoughtfully -- such that purchased lists do not exhibit problematic racial inequalities. At the same time, student list products offer customers discretion in choosing filters that introduces the possibility of individual-level racial bias (explicit or implicit) (e.g., excluding predominantly non-white zip codes) that is not possible in purely algorithmic scoring products.

Student list products share commonalities with micro-targeting and market segmentation products offered by Facebook, Google, and political consulting firms. Rather than assign a single global score, these products classify potential customers into groups in order to target each group efficiently.  However, classification systems are not neutral because they are created by people to maximize particular outcomes. To the extent that customers of student list products (universities) value the ablility to target students from affluent, predominantly white schools and communities, then student list products incorporate filters that enable universities to do this with great efficiency. Additionally, like classification products, student list products are limited by who is included in the underlying data. Student list products offered by College Board and ACT exclude students who do not take their standanrdized assessments.

To summarize, student list products exhibit systematic racial and socioeconomic disparities due to disparities in who is included in the underlying database and due to the inclusion of structurually racist search filter inputs to select prospects from the underlying database. When purchasing student lists, universities may be thoughtful about avoiding structurally racist search filters. However, individual discretion also raises the possibility of racial disparities due to explicit/implicit prejudice and also due to lack of understanding about the products they are using.

## Predicting Exclusion

@list_empirics categorize the filters available in the College Board Student Search Service product into the four buckets of geographic, academic, demographic, and student preferences (e.g., desired campus size, intended major). Table X describes filters by category. Drawing from the sociology of race, we conceptualize particular student list filters as "structurally racist" or "racialized inputs" if they have the appearance of neutrality but are correlated with race due to historic exclusion from this input. This section develops predictions about the relationship between filters and exclusion, focusing on racial and socioeconomic exclusion.

### Geographic filters

Geographic search filters (e.g., state, zip code, "geomarket," CBSA, geodemographic market segment) enable universities to target prospects based on where they live.

The concepts of "space" and "place" from critical geography [@RN4754; @RN4755] describe two alternative approaches to conceptualizing geographic location. Place refers to a holistic understanding of a geographic location that incorporates the “history, peoples, and purposes within the political, social, and economic landscape” [@RN4755, p. 317]. By contrast, space simply refers to a physical location which can be described in terms of quantifiable spatial features (e.g., distance, demographics, economic activity) and geospatial research adopts a view of space “as a location on a surface where things ‘just happen’” [@RN4754, p. 318].

Marketing practices conceive of geography as space. Market research exploits racial segregation as a means to identify and target prospective customers [@RN4772; @RN4775]. @RN4775 (p. 147) states that "racialized zip codes are the output of Jim Crow policies and the input of New Jim Code practices." Geodemography (now referred to as "spatial big data") is a branch of market research that estimates the behavior of consumers based on where they live. @RN4565, which describes the development of geodemographic segment filters, illustrates the underlying assumptions of geodemography: The basic tenet of geodemography is that people with similar cultural backgrounds, means, and perspectives naturally gravitate toward one another or form relatively homogeneous communities; in other words, birds of a feather flock together (p. 1).

Scholarship on racial segregation conceives of geography as place. Contemporary segregation is a function of historic [e.g., @rothstein2017color; @RN4551] and contemporary [e.g., @RN4801; RN4800] structurally racist laws, policies, and practices promoting residential segregation. Geographic filters are built on the back of racial segregation. We argue that targeting prospective students based on geographic location (space) without consideration to the history (place) that produces its unique patterns of residential segregation is likely to reinforce historical race-based inequality in educational opportunity.

EXPECTATIONS 
We expect that utilizing smaller geographic filters is associated with greater racial and socioeconomic dispairities in student list purchases because American residential segregation occurs at fine-grained geographic levels [CITE]. Prior research on recruiting finds that selective private universities and also public research universities -- particularly in out-of-state recruiting efforts -- target affluent schools and communities [@RN3519; @RN4759; @RN4758; @RN4733].  These results suggest that universities may filter on affluent zip codes when purchasing student lists. We expect that filtering for affluent neighborhoods is positively associated with racial exclusion because people of historical and contemporary practices that exclude people of color from living in many affluent neighborhoods

"Geomarkets" are created by by College Board Enrollment Management Services using information about score senders from previous admissions cycles [@RN1621] <!-- Geomarket filters are created by the College Board within their Enrollment Management Services, which use information about SAT score senders from the past five admissions cycles within a specific geographic locality (e.g., counties, metropolitan areas, cities) to make projections about high school graduates in the area (College Board, 2011a) -->. For example, CA10 is the "City of San Jose" and CA11 is "Santa Clara County excluding San Jose." We expect that geomarket filters positively associated with racial exclusion because geomarket borders are drawn to reflect historic geographic disparities in educational opportunities.

SOMETHING ABOUT GEODEMOGRAPHIC FILTERS?

### Academic filters

College Board academic filters include high school graduating class, SAT score, PSAT score, AP score by subject, high school GPA, and high school class rank.

Broadly, the first source of racial and socioeconomic exclusion is that the underlying student list database excludes students who have not taken College Board assessments (SAT, AP, PSAT). Prior research shows that test-taking rates differ by race and class. Thus, we expect that who is included in the underlying database exhibits racial and socioeconomic exclusion prior to conditioning on a particular assessment or a particular score. 

A related source of exclusion within the population of test-takers is who takes which assessment. Students attending schools in affluent communities tend to have better access to AP curricula. Research shows that students from underrpresented populations are less likely to take the PSAT [CITE] or AP tests [TRUE? CITE?] [note that CB did not release AP participation rates by race]. Therefore, conditioning on these assessments may increase exclusion. Additionally, College Board Search enables universities to target high school students early in the search process (e.g., sophomore PSAT takers) but students from underrepresented populations are more likely to take assessments later in high school (e.g., take SAT senior year). We expect that filtering for prospects early in high school is positively associated with racial and socioeconomic exclusion. 

A second broad source of exclusion comes from test score thresholds utilized on filters. Test scores differ by race and class as a function of differential access to test preparation and …. [READ SOME STUFF AND THEN WRITE; STUFF FROM STRUCTURAL RACISM IN STANDARDIZED TESTING BY TIAKO AND RAY?]. We expect a positive relationship between test score thresholds and racial and socioeconomic exclusion. As an alternative to test scores, universities may filter on high school GPA or high school class rank [RESEARCH BY TIENDA AND FOLKS?]. We expect a weaker relationship between these filters and racial/socioeconomic exclusion.


### Demographic filters

Demographic filters include race, ethnicity, gender, low SES, and first generation status. We focus on filtering by race, which was the most commonly used demographic filter in @list_empirics.

Drawing from critical legal scholarship [@RN4685;@RN4551], we argue that race/ethnicity filters tend to exclude students from communities of color even when they are used to target non-white prospects. @RN4551 conceptualizes "whiteness as property" as tangible, legally sectioned economic benefits that accrue to white people because of four "property functions of whiteness (rights of disposition, right to use and enjoyment; right to reputation and status; right to exclude). Whiteness and non-whiteness also define the “reputation and status” ascribed to localities, whereby “‘the inner city,’ ‘the ghetto,’ and ‘urban’ are linked to communities of color” [@RN4759, p. X]. The “absolute right to exclude is exemplified in exclusionary zoning ordinances prohibiting multi-family units) historically used to discourage Black residents from living in predominantly White areas” [@RN4759, p. X].

Whereas "nonwhiteness" was historically "used as a basis for withholding value by denying nonwhite people legal rights and privileges” [@RN4685, p. 2155], nonwhiteness now confers social and legal value as a function with society's preoccupation with diversity. The commodification of nonwhiteness -- a “commodity to be pursued, captured, possessed, and used” (p. 2155) -- encourages organizations to prioritize representational diversity, which @RN4685 argues is exemplified by universities enrolling and marketing a diverse student body as a marker of status and prestige. However, selective universities pursue representational diversity while simultaneously privileging characteristics  associated with whiteness (e.g., a "good high school", "interesting extracurricular activities", "good scores") [@RN4720;@RN4495; @RN3519]. By combining race/ethnicity filters with academic achievement (e.g., AP test score range), geographic, and/or geodemographic filters, universities can screen for Students of Color who have characteristics perceived to be associated with whiteness, often as a function of living in a predominantly white community or attending a predominantly white school.

Building from these ideas, we expect that filtering for underrepresented students of color in combination with racialized inputs (e.g., AP scores, PSAT scores, affluent zip codes) systematically excludes students of color who live in predominantly non-white communities and attend predominantly non-white schools.


### Student preference filters

### ? exclusion due to multiple racialized inputs?




# References

<div id="refs"></div>



